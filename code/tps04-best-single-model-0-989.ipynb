{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T21:01:15.301089Z","iopub.execute_input":"2022-04-29T21:01:15.301449Z","iopub.status.idle":"2022-04-29T21:01:21.856259Z","shell.execute_reply.started":"2022-04-29T21:01:15.301366Z","shell.execute_reply":"2022-04-29T21:01:21.855536Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-04-29 21:01:16.825354: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-04-29 21:01:16.825481: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Detect hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:01:55.111852Z","iopub.execute_input":"2022-04-29T21:01:55.112699Z","iopub.status.idle":"2022-04-29T21:02:00.100277Z","shell.execute_reply.started":"2022-04-29T21:01:55.112658Z","shell.execute_reply":"2022-04-29T21:02:00.099314Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2022-04-29 21:01:55.124077: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-04-29 21:01:55.127182: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-04-29 21:01:55.127217: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-04-29 21:01:55.127243: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aad6d992cd84): /proc/driver/nvidia/version does not exist\n2022-04-29 21:01:55.131556: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-29 21:01:55.132961: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-04-29 21:01:55.169310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-04-29 21:01:55.169362: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30048}\n2022-04-29 21:01:55.189590: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-04-29 21:01:55.189646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30048}\n2022-04-29 21:01:55.191274: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30048\n","output_type":"stream"},{"name":"stdout","text":"Running on TPU  ['10.0.0.2:8470']\nNumber of accelerators:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size_per_replica = 256\nbatch_size = batch_size_per_replica * strategy.num_replicas_in_sync\nbatch_size","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:02:00.101840Z","iopub.execute_input":"2022-04-29T21:02:00.102398Z","iopub.status.idle":"2022-04-29T21:02:00.111189Z","shell.execute_reply.started":"2022-04-29T21:02:00.102346Z","shell.execute_reply":"2022-04-29T21:02:00.110226Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"2048"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\nsample = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:22:51.471574Z","iopub.execute_input":"2022-04-29T21:22:51.471948Z","iopub.status.idle":"2022-04-29T21:23:03.082265Z","shell.execute_reply.started":"2022-04-29T21:22:51.471894Z","shell.execute_reply":"2022-04-29T21:23:03.081362Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def feat_eng(df):\n    \n    seq_df=pd.DataFrame()\n    sensors=[col for col in df if col.startswith('sensor')]\n    print('Processing New DF')\n    \n    temp = df.subject.value_counts().sort_values() // 60\n    subject_count = df.merge(temp, left_on='subject', right_index=True, how='left').iloc[:, -1]\n    df['subject_count'] = subject_count\n    \n    for sensor in tqdm(sensors):\n#         if sensor != 'sensor_02':\n#             df['{}_groupby_sensor_2'.format(sensor)] = df.merge(df.groupby('sensor_02')[sensor].median(), \n#                                                                  left_on='sensor_02', right_index=True, how='left').iloc[:, -1]\n#             df['{}_groupby_sensor_2_diff'.format(sensor)] = df[sensor] - df['{}_groupby_sensor_2'.format(sensor)]\n        df['{}_lag1'.format(sensor)] = df.groupby('sequence')[sensor].shift(1)\n        df['{}_lag1'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n        df['{}_diff'.format(sensor)] = df[sensor] - df['{}_lag1'.format(sensor)] \n        df['{}_roll_mean3'.format(sensor)]=df['{}'.format(sensor)].rolling(window=3).mean()\n        df['{}_roll_mean6'.format(sensor)]=df['{}'.format(sensor)].rolling(window=6).mean()\n        df['{}_roll_mean9'.format(sensor)]=df['{}'.format(sensor)].rolling(window=9).mean()\n        df['{}_roll_mean3'.format(sensor)].fillna(df['{}_roll_mean3'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean6'.format(sensor)].fillna(df['{}_roll_mean6'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean9'.format(sensor)].fillna(df['{}_roll_mean9'.format(sensor)].median(), inplace=True)\n        \n        if sensor == 'sensor_02':\n            df['{}_lag2'.format(sensor)] = df.groupby('sequence')[sensor].shift(2)\n            df['{}_lag2'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag2'.format(sensor)] = df[sensor] - df['{}_lag2'.format(sensor)]\n            df['{}_lag3'.format(sensor)] = df.groupby('sequence')[sensor].shift(3)\n            df['{}_lag3'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag3'.format(sensor)] = df[sensor] - df['{}_lag3'.format(sensor)]\n            df['{}_lag5'.format(sensor)] = df.groupby('sequence')[sensor].shift(5)\n            df['{}_lag5'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag5'.format(sensor)] = df[sensor] - df['{}_lag5'.format(sensor)]\n\n            df['{}_lead1'.format(sensor)] = df.groupby('sequence')[sensor].shift(-1)\n            df['{}_lead1'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead1'.format(sensor)] = df[sensor] - df['{}_lead1'.format(sensor)]\n            df['{}_lead2'.format(sensor)] = df.groupby('sequence')[sensor].shift(-2)\n            df['{}_lead2'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead2'.format(sensor)] = df[sensor] - df['{}_lead2'.format(sensor)]\n            df['{}_lead3'.format(sensor)] = df.groupby('sequence')[sensor].shift(-3)\n            df['{}_lead3'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead3'.format(sensor)] = df[sensor] - df['{}_lead3'.format(sensor)]\n            df['{}_lead5'.format(sensor)] = df.groupby('sequence')[sensor].shift(-5)\n            df['{}_lead5'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead5'.format(sensor)] = df[sensor] - df['{}_lead5'.format(sensor)]\n        \n        \n        \n#         aa = df.groupby(['sequence','subject'], as_index=False)[sensor].mean()\n#         aa = aa.sort_values(by=['sequence', 'subject'])\n#         train2 = train.copy()\n#         df = df.merge(aa, left_on=['sequence', 'subject'], \n#                       right_on=['sequence', 'subject'], how='left', suffixes=['', '_grouped_by_seq_sub_mean'])\n#         df['diff_'] = df[sensor] - df[f'{sensor}_grouped_by_seq_sub_mean']\n\n        if sensor == 'sensor_02':\n            df['{}_groupby_count'.format(sensor)] = df.merge(df.groupby('subject_count')[sensor].mean(), \n                                                                     left_on='subject_count', right_index=True, how='left').iloc[:, -1]\n\n            df['{}_groupby_count_diff'.format(sensor)] = df[sensor] - df['{}_groupby_count'.format(sensor)]\n        \n        s_diff='{}_diff'.format(sensor)\n        seq_df['{}_mean'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].mean()\n        seq_df['{}_diff_mean'.format(sensor)] = df.groupby(['sequence','subject'])[s_diff].mean()\n        seq_df['{}_med'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].median()\n        seq_df['{}_std'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].std()\n        seq_df['{}_skew'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].skew()\n        seq_df['{}_kurt'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].apply(pd.DataFrame.kurt)\n        seq_df['{}_min'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].min()\n        seq_df['{}_max'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].max()\n    \n    \n    bucketized_00 = pd.qcut(df['sensor_00'], q=10, labels=list(range(10)))\n    df['bucketized_00'] = bucketized_00\n    df['{}_groupby_sensor_00'.format('sensor_02')] = df.merge(df.groupby('bucketized_00')['sensor_02'].mean(), \n                                                                 left_on='bucketized_00', right_index=True, how='left').iloc[:, -1]\n    df['{}_groupby_sensor_00_diff'.format('sensor_02')] = df['sensor_02'] - df['{}_groupby_sensor_00'.format('sensor_02')]\n    df.drop('bucketized_00', axis=1, inplace=True)\n    \n    \n#     bucketized_09 = pd.qcut(df['sensor_09'], q=10, labels=list(range(10)))\n#     df['bucketized_09'] = bucketized_09\n#     df['{}_groupby_sensor_09'.format('sensor_02')] = df.merge(df.groupby('bucketized_09')['sensor_02'].mean(), \n#                                                                  left_on='bucketized_09', right_index=True, how='left').iloc[:, -1]\n#     df['{}_groupby_sensor_09_diff'.format('sensor_02')] = df['sensor_02'] - df['{}_groupby_sensor_09'.format('sensor_02')]\n#     df.drop('bucketized_09', axis=1, inplace=True)\n    \n    return df, seq_df.reset_index()\n\nwarnings.filterwarnings('ignore')\ntrain, train_seq_df =feat_eng(df=train)\ntest, test_seq_df =feat_eng(df=test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:23:03.084139Z","iopub.execute_input":"2022-04-29T21:23:03.084837Z","iopub.status.idle":"2022-04-29T21:25:49.163730Z","shell.execute_reply.started":"2022-04-29T21:23:03.084786Z","shell.execute_reply":"2022-04-29T21:25:49.162749Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Processing New DF\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [01:51<00:00,  8.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"Processing New DF\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:50<00:00,  3.88s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_seq_df.shape, test_seq_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:50.575369Z","iopub.execute_input":"2022-04-29T13:40:50.576015Z","iopub.status.idle":"2022-04-29T13:40:50.581531Z","shell.execute_reply.started":"2022-04-29T13:40:50.575953Z","shell.execute_reply":"2022-04-29T13:40:50.580586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_seq_df = train_seq_df.values\n# test_seq_df = test_seq_df.values","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:50.583284Z","iopub.execute_input":"2022-04-29T13:40:50.584267Z","iopub.status.idle":"2022-04-29T13:40:50.594281Z","shell.execute_reply.started":"2022-04-29T13:40:50.58422Z","shell.execute_reply":"2022-04-29T13:40:50.593386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.iloc[:, 3:]\ntest = test.iloc[:, 3:]\n\nlen_train = len(train)\nconcat = pd.concat([train, test], axis=0)\ndel train, test\n# time.sleep(10)\n\nscaler = StandardScaler()\nconcat_scaled = scaler.fit_transform(concat)\ndel concat \ntime.sleep(10)\n\n# train_scaled = scaler.fit_transform(train)\n# del train\n# time.sleep(10)\n\n# test_scaled = scaler.transform(test)\n# del test\ntime.sleep(10)\n\ntrain_scaled = concat_scaled[:len_train, :]\ntest_scaled = concat_scaled[len_train:, :]\n\ntrain_scaled = train_scaled.reshape(-1, 60, train_scaled.shape[-1])\ntest_scaled = test_scaled.reshape(-1, 60, train_scaled.shape[-1])\ntrain_scaled.shape, test_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:25:49.165376Z","iopub.execute_input":"2022-04-29T21:25:49.165719Z","iopub.status.idle":"2022-04-29T21:26:18.394602Z","shell.execute_reply.started":"2022-04-29T21:25:49.165687Z","shell.execute_reply":"2022-04-29T21:26:18.393664Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((25968, 60, 97), (12218, 60, 97))"},"metadata":{}}]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n#     random_mask = layers.Input(shape=(num_heads, 60, 60))\n#     indexes = tf.range(inputs.shape[-2])\n#     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n#     encoded_inputs = inputs + pos_encoding\n    \n#     encoder = trasformer_block(encoded_inputs, linear_shape=linear_shape, num_heads=num_heads, attention_mask=random_mask)\n#     encoder = 0.3*encoded_inputs + 0.7*encoder\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, attention_mask=random_mask)\n#             encoder = 0.3*x + 0.7*encoder\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n            \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.0)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=[inputs, random_mask], outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=0.0), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.399359Z","iopub.execute_input":"2022-04-29T13:40:59.399598Z","iopub.status.idle":"2022-04-29T13:40:59.404116Z","shell.execute_reply.started":"2022-04-29T13:40:59.399565Z","shell.execute_reply":"2022-04-29T13:40:59.403427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nLR_START = 1e-4\nLR_MAX = 1e-3\nLR_MIN = 5e-5\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 50\nSTEPS = [50]\n\n\ndef lrfn(epoch):\n    if epoch<STEPS[0]:\n        epoch2 = epoch\n        EPOCHS2 = STEPS[0]\n    elif epoch<STEPS[0]+STEPS[1]:\n        epoch2 = epoch-STEPS[0]\n        EPOCHS2 = STEPS[1]\n    elif epoch<STEPS[0]+STEPS[1]+STEPS[2]:\n        epoch2 = epoch-STEPS[0]-STEPS[1]\n        EPOCHS2 = STEPS[2]\n    \n    if epoch2 < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch2 + LR_START\n    elif epoch2 < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:38:03.023268Z","iopub.execute_input":"2022-04-29T21:38:03.023573Z","iopub.status.idle":"2022-04-29T21:38:03.046936Z","shell.execute_reply.started":"2022-04-29T21:38:03.023544Z","shell.execute_reply":"2022-04-29T21:38:03.046131Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# def model():\n#     x_input = layers.Input(shape=(train_scaled.shape[-2:]))\n#     x1 = layers.Bidirectional(layers.LSTM(units=512, return_sequences=True))(x_input)\n\n#     l1 = layers.Bidirectional(layers.LSTM(units=384, return_sequences=True))(x1)\n#     l2 = layers.Bidirectional(layers.LSTM(units=384, return_sequences=True))(x_input)\n\n#     c1 = layers.Concatenate(axis=2)([l1,l2])\n\n#     l3 = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(c1)\n#     l4 = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = layers.Concatenate(axis=2)([l3,l4])\n\n#     l6 = layers.GlobalMaxPooling1D()(c2)\n#     l7 = layers.Dense(units=128, activation='selu')(l6)\n#     l8 = layers.Dropout(0.05)(l7)\n\n#     output = layers.Dense(1, activation='sigmoid')(l8)\n    \n#     model = tf.keras.Model(inputs=x_input, outputs=output)\n#     model.compile(optimizer='adam', \n#                   loss='binary_crossentropy', \n#                   metrics=[tf.keras.metrics.AUC(name = 'auc')])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.426353Z","iopub.execute_input":"2022-04-29T13:40:59.42671Z","iopub.status.idle":"2022-04-29T13:40:59.437857Z","shell.execute_reply.started":"2022-04-29T13:40:59.426675Z","shell.execute_reply":"2022-04-29T13:40:59.437199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = transformer_encoder(num_blocks=1, linear_shape=64, num_heads=4)\n# model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.439233Z","iopub.execute_input":"2022-04-29T13:40:59.44018Z","iopub.status.idle":"2022-04-29T13:40:59.4535Z","shell.execute_reply.started":"2022-04-29T13:40:59.440133Z","shell.execute_reply":"2022-04-29T13:40:59.452642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with strategy.scope():\n#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n#     train_metric = tf.keras.metrics.AUC(name='auc')\n#     valid_metric = tf.keras.metrics.AUC(name='auc')\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n# @tf.function\n# def train_step(x, y):\n#     with tf.GradientTape() as tape:\n#         preds = model(x, training=True)\n#         loss_value = loss_object(y, preds)\n#     grads = tape.gradient(loss_value, model.trainable_weights)\n#     optimizer.apply_gradients((zip(grads, model.trainable_weights)))\n#     train_metric.update_state(y, preds)\n#     return loss_value\n\n# @tf.function\n# def valid_step(x, y):\n#     preds = model(x, training=True)\n#     loss_value = loss_object(y, preds)\n#     valid_metric.update_state(y, preds)\n#     return loss_value\n\n\n# n_splits = 3\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n#     train_ds = train_ds.shuffle(1024)\n#     train_ds = train_ds.batch(128).prefetch(-1)\n    \n#     valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n#     valid_ds = valid_ds.batch(128).prefetch(-1)\n    \n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.2)\n    \n#     for epoch in range(100):\n#         print(f'Epoch --------> {epoch+1}')\n#         train_loss_list = []\n#         epoch_random_mask = tf.random.uniform(shape=(1, 60, 60)) + 0.2\n#         epoch_random_mask = tf.broadcast_to(epoch_random_mask, shape=(8, 60, 60))\n#         epoch_random_mask = tf.where(epoch_random_mask<0.5, 0.0, 1.0)\n#         for x, y in tqdm(train_ds, total=len(train_ds)):\n#             random_mask = tf.broadcast_to(epoch_random_mask, shape=(x.shape[0], 8, 60, 60))\n#             inputs = (x, random_mask)\n#             train_loss_list.append(train_step(inputs, y))\n#         print('Train', 'Loss:', np.mean(train_loss_list), 'AUC:', train_metric.result().numpy())\n#         train_metric.reset_state()\n        \n#         valid_loss_list = []\n#         for x, y in tqdm(valid_ds, total=len(valid_ds)):\n#             ones_mask = tf.ones(shape=(8, 60, 60))\n#             ones_mask = tf.broadcast_to(ones_mask, shape=(x.shape[0], 8, 60, 60))\n#             inputs = (x, ones_mask)\n#             valid_loss_list.append(valid_step(inputs, y))\n#         print('Valid', 'Loss:', np.mean(valid_loss_list), 'AUC:', valid_metric.result().numpy())\n#         valid_metric.reset_state()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.455431Z","iopub.execute_input":"2022-04-29T13:40:59.45586Z","iopub.status.idle":"2022-04-29T13:40:59.466795Z","shell.execute_reply.started":"2022-04-29T13:40:59.455808Z","shell.execute_reply":"2022-04-29T13:40:59.465774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def trasformer_block(inputs, linear_shape=512, num_heads=8, dropout_rate=0.0):\n#     x = layers.MultiHeadAttention(num_heads=num_heads,\n#                                   key_dim=linear_shape,\n#                                   value_dim=linear_shape,\n#                                   dropout=dropout_rate)(inputs, inputs)\n#     x = layers.Add()([inputs, x])\n#     x1 = layers.LayerNormalization()(x)\n    \n#     x = layers.Dense(linear_shape, activation='gelu')(x1)\n#     x = layers.Dense(inputs.shape[-1])(x)\n    \n#     x = layers.Add()([x1, x])\n#     x2 = layers.LayerNormalization()(x)\n#     return x2","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.468347Z","iopub.execute_input":"2022-04-29T13:40:59.470331Z","iopub.status.idle":"2022-04-29T13:40:59.482224Z","shell.execute_reply.started":"2022-04-29T13:40:59.470281Z","shell.execute_reply":"2022-04-29T13:40:59.481422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n#     indexes = tf.range(inputs.shape[-2])\n#     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n#     encoded_inputs = inputs + pos_encoding\n    \n#     encoder = trasformer_block(encoded_inputs, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #     encoder = 0.3*encoded_inputs + 0.7*encoder\n#     encoder = layers.BatchNormalization()(encoder)\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #             encoder = 0.3*x + 0.7*encoder\n#             encoder = layers.BatchNormalization()(encoder)\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n            \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.5)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=5e-4), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.483689Z","iopub.execute_input":"2022-04-29T13:40:59.483956Z","iopub.status.idle":"2022-04-29T13:40:59.499143Z","shell.execute_reply.started":"2022-04-29T13:40:59.483925Z","shell.execute_reply":"2022-04-29T13:40:59.498187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n# #     indexes = tf.range(inputs.shape[-2])\n# #     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n# #     encoded_inputs = inputs + pos_encoding\n    \n#     lstm = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(inputs)\n#     lstm = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(lstm)\n    \n#     encoder = trasformer_block(lstm, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #     encoder = lstm + encoder\n#     encoder = layers.BatchNormalization()(encoder)\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #             encoder = x + encoder\n#             encoder = layers.BatchNormalization()(encoder)\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n        \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.5)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=5e-4), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.503071Z","iopub.execute_input":"2022-04-29T13:40:59.503332Z","iopub.status.idle":"2022-04-29T13:40:59.513793Z","shell.execute_reply.started":"2022-04-29T13:40:59.503303Z","shell.execute_reply":"2022-04-29T13:40:59.512886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = transformer_encoder(num_blocks=2, linear_shape=64, num_heads=4, dropout_rate=0.0)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.515466Z","iopub.execute_input":"2022-04-29T13:40:59.515709Z","iopub.status.idle":"2022-04-29T13:40:59.528816Z","shell.execute_reply.started":"2022-04-29T13:40:59.515681Z","shell.execute_reply":"2022-04-29T13:40:59.528106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_splits = 3\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=10, verbose=True)\n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=2, linear_shape=32, num_heads=8, dropout_rate=0.0)\n#     model.fit(x_train, y_train, \n#               validation_data=(x_valid, y_valid),\n#               epochs=120, batch_size=256, callbacks=[ckpt])\n#     break","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.530356Z","iopub.execute_input":"2022-04-29T13:40:59.530851Z","iopub.status.idle":"2022-04-29T13:40:59.541985Z","shell.execute_reply.started":"2022-04-29T13:40:59.530806Z","shell.execute_reply":"2022-04-29T13:40:59.540923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=4, verbose=True, mode='max')\n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=4, linear_shape=64, num_heads=4, dropout_rate=0.0)\n#     model.fit(x_train, y_train, \n#               validation_data=(x_valid, y_valid),\n#               epochs=20, batch_size=256, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.543493Z","iopub.execute_input":"2022-04-29T13:40:59.544525Z","iopub.status.idle":"2022-04-29T13:40:59.558475Z","shell.execute_reply.started":"2022-04-29T13:40:59.544483Z","shell.execute_reply":"2022-04-29T13:40:59.557566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dense_block(inputs):\n#     x = layers.Dense(256, activation='gelu')(inputs)\n#     x = layers.Dense(256, activation='gelu')(x)\n#     x = layers.Dense(128, activation='gelu')(x)\n#     x = layers.Dense(128, activation='gelu')(x)\n#     x = layers.Dense(64, activation='gelu')(x)\n#     return x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.560331Z","iopub.execute_input":"2022-04-29T13:40:59.560921Z","iopub.status.idle":"2022-04-29T13:40:59.574601Z","shell.execute_reply.started":"2022-04-29T13:40:59.560876Z","shell.execute_reply":"2022-04-29T13:40:59.573858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n#     dense = Dense(128, activation='gelu')(inputs)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n# #     dense = layers.Add()([x_input, dense])\n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(dense)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(inputs)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = Bidirectional(LSTM(units=256, return_sequences=True))(c1)\n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = Concatenate(axis=2)([l3,l4])\n\n#     l6 = GlobalMaxPooling1D()(c2)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.576344Z","iopub.execute_input":"2022-04-29T13:40:59.577197Z","iopub.status.idle":"2022-04-29T13:40:59.585856Z","shell.execute_reply.started":"2022-04-29T13:40:59.577147Z","shell.execute_reply":"2022-04-29T13:40:59.58514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnn_block(inputs, filters=128, dropout=0.4):\n    x = layers.Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:32:23.631635Z","iopub.execute_input":"2022-04-29T21:32:23.632000Z","iopub.status.idle":"2022-04-29T21:32:23.638790Z","shell.execute_reply.started":"2022-04-29T21:32:23.631963Z","shell.execute_reply":"2022-04-29T21:32:23.637892Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class GeMPoolingLayer(tf.keras.layers.Layer):\n    def __init__(self, p=1., train_p=False):\n        super().__init__()\n        if train_p:\n            self.p = tf.Variable(p, dtype=tf.float32)\n        else:\n            self.p = p\n        self.eps = 1e-6\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n        inputs = tf.pow(inputs, self.p)\n        inputs = tf.reduce_mean(inputs, axis=[1], keepdims=False)\n        inputs = tf.pow(inputs, 1./self.p)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:32:23.961003Z","iopub.execute_input":"2022-04-29T21:32:23.961834Z","iopub.status.idle":"2022-04-29T21:32:23.970424Z","shell.execute_reply.started":"2022-04-29T21:32:23.961785Z","shell.execute_reply":"2022-04-29T21:32:23.969480Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\ndef lstm_model():\n    inputs = Input(shape=(train_scaled.shape[-2:]))\n#     dense = Dense(128, activation='gelu')(inputs)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n    processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n    cnn = cnn_block(processed, filters=128, dropout=0.40)\n    cnn = cnn_block(cnn, filters=64, dropout=0.35)\n    cnn = cnn_block(cnn, filters=32, dropout=0.20)\n    cnn = cnn_block(cnn, filters=1, dropout=0.0)\n    cnn = layers.MaxPooling2D()(cnn)\n    cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn)\n    \n    x1 = Bidirectional(GRU(units=512, return_sequences=True))(cnn)\n\n    l1 = Bidirectional(GRU(units=384, return_sequences=True))(x1)\n    l2 = Bidirectional(GRU(units=384, return_sequences=True))(cnn)\n\n    c1 = Concatenate(axis=2)([l1,l2])\n\n    l3 = Bidirectional(GRU(units=256, return_sequences=True))(c1)\n    l4 = Bidirectional(GRU(units=256, return_sequences=True))(l2)\n\n    c2 = Concatenate(axis=2)([l3,l4])\n#     c2 = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c2)\n#     c2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, activation='gelu', padding='valid')(c2)\n#     c2 = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(c2)\n    l6 = GlobalMaxPooling1D()(c2)\n#     l6 = GeMPoolingLayer(p=1., train_p=True)(c2)\n    \n    l7 = Dense(units=128, activation='gelu')(l6)\n    l8 = Dropout(0.0)(l7)\n\n    output = Dense(1, activation='sigmoid')(l8)\n    \n    \n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    metric1 = tf.keras.metrics.AUC(name='auc')\n    metric2 = tf.keras.metrics.BinaryAccuracy()\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n                  metrics=[metric1, metric2])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:08:23.536171Z","iopub.execute_input":"2022-04-29T22:08:23.536464Z","iopub.status.idle":"2022-04-29T22:08:23.553318Z","shell.execute_reply.started":"2022-04-29T22:08:23.536434Z","shell.execute_reply":"2022-04-29T22:08:23.552298Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n# #     dense = Dense(128, activation='gelu')(inputs)\n# #     dense = Dense(128, activation='gelu')(dense)\n# #     dropout = Dropout(0.35)(dense)\n# #     dense = Dense(128, activation='linear')(dropout)\n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n#     cnn = cnn_block(processed, filters=128, dropout=0.40)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.35)\n#     cnn = cnn_block(cnn, filters=32, dropout=0.20)\n#     cnn = cnn_block(cnn, filters=1, dropout=0.0)\n#     cnn = layers.MaxPooling2D()(cnn)\n#     cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn)\n    \n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(cnn)\n    \n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(cnn)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = layers.MultiHeadAttention(num_heads=4,\n#                                   key_dim=64,\n#                                   value_dim=64,\n#                                   dropout=0.0)(c1, c1)\n    \n#     l3 = layers.MultiHeadAttention(num_heads=4,\n#                                   key_dim=64,\n#                                   value_dim=64,\n#                                   dropout=0.0)(l3, l3)\n    \n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l3)\n\n# #     c2 = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c2)\n# #     c2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, activation='gelu', padding='valid')(c2)\n# #     c2 = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(c2)\n#     l6 = GeMPoolingLayer(p=1., train_p=True)(l4)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.635657Z","iopub.execute_input":"2022-04-29T13:40:59.635989Z","iopub.status.idle":"2022-04-29T13:40:59.649463Z","shell.execute_reply.started":"2022-04-29T13:40:59.635944Z","shell.execute_reply":"2022-04-29T13:40:59.648538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n# #     dense = Dense(128, activation='gelu')(inputs)\n# #     dense = Dense(128, activation='gelu')(dense)\n# #     dropout = Dropout(0.35)(dense)\n# #     dense = Dense(128, activation='linear')(dropout)\n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n#     cnn = cnn_block(processed, filters=128, dropout=0.20)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = cnn_block(cnn, filters=32, dropout=0.05)\n#     cnn = cnn_block(cnn, filters=1, dropout=0.0)\n#     cnn = layers.MaxPooling2D()(cnn)\n#     cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn) \n    \n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(cnn)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(cnn)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n    \n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c1)\n#     cnn = cnn_block(processed, filters=128, dropout=0.30)\n#     cnn = cnn_block(cnn, filters=128, dropout=0.25)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = layers.MaxPooling2D()(cnn)\n\n#     l6 = layers.GlobalMaxPooling2D()(cnn)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.651234Z","iopub.execute_input":"2022-04-29T13:40:59.651734Z","iopub.status.idle":"2022-04-29T13:40:59.665713Z","shell.execute_reply.started":"2022-04-29T13:40:59.6517Z","shell.execute_reply":"2022-04-29T13:40:59.664906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = lstm_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.666805Z","iopub.execute_input":"2022-04-29T13:40:59.667755Z","iopub.status.idle":"2022-04-29T13:41:02.57086Z","shell.execute_reply.started":"2022-04-29T13:40:59.667711Z","shell.execute_reply":"2022-04-29T13:41:02.569844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout\n\n# def lstm_model():\n#     inputs_1 = Input(shape=(train_scaled.shape[-2:]))\n#     inputs_2 = Input(shape=(train_seq_df.shape[-1],))\n#     dense = Dense(128, activation='gelu')(inputs_1)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n# #     dense = layers.Add()([x_input, dense])\n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(dense)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(inputs_1)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = Bidirectional(LSTM(units=256, return_sequences=True))(c1)\n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = Concatenate(axis=2)([l3,l4])\n\n#     l6 = GlobalMaxPooling1D()(c2)\n    \n#     dense_outputs = dense_block(inputs_2)\n#     c3 = Concatenate()([l6, dense_outputs])\n    \n#     l7 = Dense(units=128, activation='gelu')(c3)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=[inputs_1, inputs_2], outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:41:02.572347Z","iopub.execute_input":"2022-04-29T13:41:02.572605Z","iopub.status.idle":"2022-04-29T13:41:02.578168Z","shell.execute_reply.started":"2022-04-29T13:41:02.572573Z","shell.execute_reply":"2022-04-29T13:41:02.577324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = lstm_model()\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:41:02.581976Z","iopub.execute_input":"2022-04-29T13:41:02.582248Z","iopub.status.idle":"2022-04-29T13:41:02.589592Z","shell.execute_reply.started":"2022-04-29T13:41:02.582217Z","shell.execute_reply":"2022-04-29T13:41:02.58895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n    for rep in range(3):\n        print('*'*30, f'Fold {fold+1} Trial {rep+1}', '*'*30)\n        x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n        x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n\n        file_path = f'Fold_{fold+1}_{rep+1}_weights.h5'\n        ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n                                                  monitor='val_auc',\n                                                  mode='max',\n                                                  save_best_only=True,\n                                                  save_weights_only=True,\n                                                 verbose=1)\n\n        tf.keras.backend.clear_session()\n        lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,  patience=4, verbose=True, mode='max')\n        with strategy.scope():\n            model = lstm_model()\n        model.fit(x_train, y_train, \n                  validation_data=(x_valid, y_valid),\n                  epochs=40, batch_size=32*8, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T22:08:32.927503Z","iopub.execute_input":"2022-04-29T22:08:32.927810Z","iopub.status.idle":"2022-04-29T23:20:52.830854Z","shell.execute_reply.started":"2022-04-29T22:08:32.927779Z","shell.execute_reply":"2022-04-29T23:20:52.829934Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"****************************** Fold 1 Trial 1 ******************************\nEpoch 1/40\n82/82 [==============================] - 53s 294ms/step - loss: 0.6403 - auc: 0.7183 - binary_accuracy: 0.6615 - val_loss: 1.4195 - val_auc: 0.7729 - val_binary_accuracy: 0.5162\n\nEpoch 00001: val_auc improved from -inf to 0.77285, saving model to Fold_1_1_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2941 - auc: 0.9469 - binary_accuracy: 0.8738 - val_loss: 1.3360 - val_auc: 0.7788 - val_binary_accuracy: 0.5293\n\nEpoch 00002: val_auc improved from 0.77285 to 0.77884, saving model to Fold_1_1_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2133 - auc: 0.9719 - binary_accuracy: 0.9130 - val_loss: 1.3010 - val_auc: 0.8665 - val_binary_accuracy: 0.5458\n\nEpoch 00003: val_auc improved from 0.77884 to 0.86654, saving model to Fold_1_1_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1946 - auc: 0.9768 - binary_accuracy: 0.9220 - val_loss: 0.9197 - val_auc: 0.8908 - val_binary_accuracy: 0.6030\n\nEpoch 00004: val_auc improved from 0.86654 to 0.89083, saving model to Fold_1_1_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.1828 - auc: 0.9799 - binary_accuracy: 0.9282 - val_loss: 0.6932 - val_auc: 0.9139 - val_binary_accuracy: 0.7023\n\nEpoch 00005: val_auc improved from 0.89083 to 0.91388, saving model to Fold_1_1_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 74ms/step - loss: 0.1659 - auc: 0.9832 - binary_accuracy: 0.9330 - val_loss: 0.4126 - val_auc: 0.9452 - val_binary_accuracy: 0.8258\n\nEpoch 00006: val_auc improved from 0.91388 to 0.94523, saving model to Fold_1_1_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 72ms/step - loss: 0.1375 - auc: 0.9884 - binary_accuracy: 0.9455 - val_loss: 0.3908 - val_auc: 0.9542 - val_binary_accuracy: 0.8531\n\nEpoch 00007: val_auc improved from 0.94523 to 0.95422, saving model to Fold_1_1_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1420 - auc: 0.9874 - binary_accuracy: 0.9452 - val_loss: 0.2652 - val_auc: 0.9655 - val_binary_accuracy: 0.8978\n\nEpoch 00008: val_auc improved from 0.95422 to 0.96546, saving model to Fold_1_1_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.1620 - auc: 0.9845 - binary_accuracy: 0.9324 - val_loss: 0.2555 - val_auc: 0.9726 - val_binary_accuracy: 0.8987\n\nEpoch 00009: val_auc improved from 0.96546 to 0.97259, saving model to Fold_1_1_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1303 - auc: 0.9895 - binary_accuracy: 0.9496 - val_loss: 0.2160 - val_auc: 0.9767 - val_binary_accuracy: 0.9176\n\nEpoch 00010: val_auc improved from 0.97259 to 0.97672, saving model to Fold_1_1_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1763 - auc: 0.9818 - binary_accuracy: 0.9282 - val_loss: 0.2422 - val_auc: 0.9755 - val_binary_accuracy: 0.9049\n\nEpoch 00011: val_auc did not improve from 0.97672\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1262 - auc: 0.9902 - binary_accuracy: 0.9506 - val_loss: 0.1866 - val_auc: 0.9795 - val_binary_accuracy: 0.9203\n\nEpoch 00012: val_auc improved from 0.97672 to 0.97954, saving model to Fold_1_1_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1288 - auc: 0.9897 - binary_accuracy: 0.9486 - val_loss: 0.2074 - val_auc: 0.9793 - val_binary_accuracy: 0.9153\n\nEpoch 00013: val_auc did not improve from 0.97954\nEpoch 14/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1177 - auc: 0.9914 - binary_accuracy: 0.9567 - val_loss: 0.2374 - val_auc: 0.9764 - val_binary_accuracy: 0.9016\n\nEpoch 00014: val_auc did not improve from 0.97954\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1255 - auc: 0.9904 - binary_accuracy: 0.9491 - val_loss: 0.2139 - val_auc: 0.9782 - val_binary_accuracy: 0.9157\n\nEpoch 00015: val_auc did not improve from 0.97954\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1210 - auc: 0.9910 - binary_accuracy: 0.9529 - val_loss: 0.2349 - val_auc: 0.9735 - val_binary_accuracy: 0.9087\n\nEpoch 00016: val_auc did not improve from 0.97954\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0967 - auc: 0.9940 - binary_accuracy: 0.9639 - val_loss: 0.2448 - val_auc: 0.9762 - val_binary_accuracy: 0.9062\n\nEpoch 00017: val_auc did not improve from 0.97954\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0886 - auc: 0.9951 - binary_accuracy: 0.9664 - val_loss: 0.2012 - val_auc: 0.9788 - val_binary_accuracy: 0.9197\n\nEpoch 00018: val_auc did not improve from 0.97954\nEpoch 19/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0851 - auc: 0.9955 - binary_accuracy: 0.9662 - val_loss: 0.2221 - val_auc: 0.9755 - val_binary_accuracy: 0.9130\n\nEpoch 00019: val_auc did not improve from 0.97954\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0788 - auc: 0.9961 - binary_accuracy: 0.9701 - val_loss: 0.2358 - val_auc: 0.9767 - val_binary_accuracy: 0.9136\n\nEpoch 00020: val_auc did not improve from 0.97954\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0706 - auc: 0.9968 - binary_accuracy: 0.9753 - val_loss: 0.2351 - val_auc: 0.9792 - val_binary_accuracy: 0.9095\n\nEpoch 00021: val_auc did not improve from 0.97954\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0747 - auc: 0.9967 - binary_accuracy: 0.9715 - val_loss: 0.1992 - val_auc: 0.9805 - val_binary_accuracy: 0.9251\n\nEpoch 00022: val_auc improved from 0.97954 to 0.98049, saving model to Fold_1_1_weights.h5\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0669 - auc: 0.9973 - binary_accuracy: 0.9743 - val_loss: 0.2603 - val_auc: 0.9754 - val_binary_accuracy: 0.9039\n\nEpoch 00023: val_auc did not improve from 0.98049\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0677 - auc: 0.9972 - binary_accuracy: 0.9761 - val_loss: 0.2247 - val_auc: 0.9776 - val_binary_accuracy: 0.9149\n\nEpoch 00024: val_auc did not improve from 0.98049\nEpoch 25/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0804 - auc: 0.9961 - binary_accuracy: 0.9691 - val_loss: 0.1824 - val_auc: 0.9819 - val_binary_accuracy: 0.9290\n\nEpoch 00025: val_auc improved from 0.98049 to 0.98194, saving model to Fold_1_1_weights.h5\nEpoch 26/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0770 - auc: 0.9963 - binary_accuracy: 0.9713 - val_loss: 0.1843 - val_auc: 0.9815 - val_binary_accuracy: 0.9268\n\nEpoch 00026: val_auc did not improve from 0.98194\nEpoch 27/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0776 - auc: 0.9964 - binary_accuracy: 0.9708 - val_loss: 0.1705 - val_auc: 0.9833 - val_binary_accuracy: 0.9345\n\nEpoch 00027: val_auc improved from 0.98194 to 0.98331, saving model to Fold_1_1_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0813 - auc: 0.9960 - binary_accuracy: 0.9696 - val_loss: 0.1696 - val_auc: 0.9836 - val_binary_accuracy: 0.9336\n\nEpoch 00028: val_auc improved from 0.98331 to 0.98356, saving model to Fold_1_1_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0847 - auc: 0.9955 - binary_accuracy: 0.9683 - val_loss: 0.1645 - val_auc: 0.9842 - val_binary_accuracy: 0.9336\n\nEpoch 00029: val_auc improved from 0.98356 to 0.98418, saving model to Fold_1_1_weights.h5\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0878 - auc: 0.9952 - binary_accuracy: 0.9672 - val_loss: 0.1571 - val_auc: 0.9850 - val_binary_accuracy: 0.9386\n\nEpoch 00030: val_auc improved from 0.98418 to 0.98504, saving model to Fold_1_1_weights.h5\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0881 - auc: 0.9954 - binary_accuracy: 0.9667 - val_loss: 0.1833 - val_auc: 0.9825 - val_binary_accuracy: 0.9263\n\nEpoch 00031: val_auc did not improve from 0.98504\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1000 - auc: 0.9938 - binary_accuracy: 0.9625 - val_loss: 0.1778 - val_auc: 0.9842 - val_binary_accuracy: 0.9299\n\nEpoch 00032: val_auc did not improve from 0.98504\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1005 - auc: 0.9938 - binary_accuracy: 0.9613 - val_loss: 0.1703 - val_auc: 0.9843 - val_binary_accuracy: 0.9326\n\nEpoch 00033: val_auc did not improve from 0.98504\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0898 - auc: 0.9951 - binary_accuracy: 0.9646 - val_loss: 0.1650 - val_auc: 0.9845 - val_binary_accuracy: 0.9340\n\nEpoch 00034: val_auc did not improve from 0.98504\n\nEpoch 00034: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 35/40\n82/82 [==============================] - 7s 80ms/step - loss: 0.0859 - auc: 0.9956 - binary_accuracy: 0.9697 - val_loss: 0.1661 - val_auc: 0.9841 - val_binary_accuracy: 0.9313\n\nEpoch 00035: val_auc did not improve from 0.98504\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0908 - auc: 0.9954 - binary_accuracy: 0.9658 - val_loss: 0.1614 - val_auc: 0.9842 - val_binary_accuracy: 0.9361\n\nEpoch 00036: val_auc did not improve from 0.98504\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1028 - auc: 0.9938 - binary_accuracy: 0.9615 - val_loss: 0.1671 - val_auc: 0.9837 - val_binary_accuracy: 0.9305\n\nEpoch 00037: val_auc did not improve from 0.98504\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1021 - auc: 0.9939 - binary_accuracy: 0.9594 - val_loss: 0.1678 - val_auc: 0.9837 - val_binary_accuracy: 0.9313\n\nEpoch 00038: val_auc did not improve from 0.98504\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1077 - auc: 0.9932 - binary_accuracy: 0.9583 - val_loss: 0.1670 - val_auc: 0.9839 - val_binary_accuracy: 0.9290\n\nEpoch 00039: val_auc did not improve from 0.98504\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1204 - auc: 0.9918 - binary_accuracy: 0.9550 - val_loss: 0.1706 - val_auc: 0.9829 - val_binary_accuracy: 0.9301\n\nEpoch 00040: val_auc did not improve from 0.98504\n****************************** Fold 1 Trial 2 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 298ms/step - loss: 0.6160 - auc: 0.7354 - binary_accuracy: 0.6758 - val_loss: 1.9194 - val_auc: 0.5158 - val_binary_accuracy: 0.4996\n\nEpoch 00001: val_auc improved from -inf to 0.51578, saving model to Fold_1_2_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3134 - auc: 0.9377 - binary_accuracy: 0.8677 - val_loss: 1.7984 - val_auc: 0.5271 - val_binary_accuracy: 0.4987\n\nEpoch 00002: val_auc improved from 0.51578 to 0.52712, saving model to Fold_1_2_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2488 - auc: 0.9622 - binary_accuracy: 0.8994 - val_loss: 2.0854 - val_auc: 0.5978 - val_binary_accuracy: 0.4990\n\nEpoch 00003: val_auc improved from 0.52712 to 0.59784, saving model to Fold_1_2_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2120 - auc: 0.9723 - binary_accuracy: 0.9159 - val_loss: 1.6102 - val_auc: 0.6559 - val_binary_accuracy: 0.5064\n\nEpoch 00004: val_auc improved from 0.59784 to 0.65590, saving model to Fold_1_2_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2119 - auc: 0.9725 - binary_accuracy: 0.9141 - val_loss: 1.9269 - val_auc: 0.6998 - val_binary_accuracy: 0.5069\n\nEpoch 00005: val_auc improved from 0.65590 to 0.69980, saving model to Fold_1_2_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1896 - auc: 0.9782 - binary_accuracy: 0.9237 - val_loss: 1.4454 - val_auc: 0.7977 - val_binary_accuracy: 0.5593\n\nEpoch 00006: val_auc improved from 0.69980 to 0.79772, saving model to Fold_1_2_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1810 - auc: 0.9797 - binary_accuracy: 0.9268 - val_loss: 0.6693 - val_auc: 0.8983 - val_binary_accuracy: 0.7143\n\nEpoch 00007: val_auc improved from 0.79772 to 0.89831, saving model to Fold_1_2_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1859 - auc: 0.9789 - binary_accuracy: 0.9252 - val_loss: 0.8835 - val_auc: 0.9081 - val_binary_accuracy: 0.7133\n\nEpoch 00008: val_auc improved from 0.89831 to 0.90815, saving model to Fold_1_2_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1697 - auc: 0.9821 - binary_accuracy: 0.9315 - val_loss: 0.2964 - val_auc: 0.9517 - val_binary_accuracy: 0.8735\n\nEpoch 00009: val_auc improved from 0.90815 to 0.95173, saving model to Fold_1_2_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1759 - auc: 0.9815 - binary_accuracy: 0.9307 - val_loss: 0.4993 - val_auc: 0.9408 - val_binary_accuracy: 0.8055\n\nEpoch 00010: val_auc did not improve from 0.95173\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1513 - auc: 0.9855 - binary_accuracy: 0.9422 - val_loss: 0.8039 - val_auc: 0.9282 - val_binary_accuracy: 0.7081\n\nEpoch 00011: val_auc did not improve from 0.95173\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1740 - auc: 0.9815 - binary_accuracy: 0.9297 - val_loss: 0.3885 - val_auc: 0.9529 - val_binary_accuracy: 0.8469\n\nEpoch 00012: val_auc improved from 0.95173 to 0.95285, saving model to Fold_1_2_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1472 - auc: 0.9866 - binary_accuracy: 0.9428 - val_loss: 0.4322 - val_auc: 0.9590 - val_binary_accuracy: 0.8392\n\nEpoch 00013: val_auc improved from 0.95285 to 0.95903, saving model to Fold_1_2_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1527 - auc: 0.9858 - binary_accuracy: 0.9412 - val_loss: 0.2882 - val_auc: 0.9703 - val_binary_accuracy: 0.8887\n\nEpoch 00014: val_auc improved from 0.95903 to 0.97032, saving model to Fold_1_2_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1346 - auc: 0.9886 - binary_accuracy: 0.9481 - val_loss: 0.2326 - val_auc: 0.9693 - val_binary_accuracy: 0.9060\n\nEpoch 00015: val_auc did not improve from 0.97032\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1443 - auc: 0.9875 - binary_accuracy: 0.9417 - val_loss: 0.5063 - val_auc: 0.9348 - val_binary_accuracy: 0.8052\n\nEpoch 00016: val_auc did not improve from 0.97032\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1286 - auc: 0.9897 - binary_accuracy: 0.9488 - val_loss: 0.2657 - val_auc: 0.9677 - val_binary_accuracy: 0.8958\n\nEpoch 00017: val_auc did not improve from 0.97032\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1260 - auc: 0.9902 - binary_accuracy: 0.9516 - val_loss: 0.3749 - val_auc: 0.9577 - val_binary_accuracy: 0.8562\n\nEpoch 00018: val_auc did not improve from 0.97032\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 19/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1140 - auc: 0.9921 - binary_accuracy: 0.9541 - val_loss: 0.3858 - val_auc: 0.9604 - val_binary_accuracy: 0.8629\n\nEpoch 00019: val_auc did not improve from 0.97032\nEpoch 20/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0959 - auc: 0.9942 - binary_accuracy: 0.9612 - val_loss: 0.4030 - val_auc: 0.9651 - val_binary_accuracy: 0.8533\n\nEpoch 00020: val_auc did not improve from 0.97032\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0960 - auc: 0.9943 - binary_accuracy: 0.9641 - val_loss: 0.3150 - val_auc: 0.9605 - val_binary_accuracy: 0.8652\n\nEpoch 00021: val_auc did not improve from 0.97032\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1060 - auc: 0.9929 - binary_accuracy: 0.9592 - val_loss: 0.3817 - val_auc: 0.9536 - val_binary_accuracy: 0.8604\n\nEpoch 00022: val_auc did not improve from 0.97032\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 23/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1036 - auc: 0.9933 - binary_accuracy: 0.9620 - val_loss: 0.3282 - val_auc: 0.9647 - val_binary_accuracy: 0.8737\n\nEpoch 00023: val_auc did not improve from 0.97032\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0892 - auc: 0.9953 - binary_accuracy: 0.9649 - val_loss: 0.3757 - val_auc: 0.9622 - val_binary_accuracy: 0.8573\n\nEpoch 00024: val_auc did not improve from 0.97032\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0868 - auc: 0.9954 - binary_accuracy: 0.9665 - val_loss: 0.2523 - val_auc: 0.9692 - val_binary_accuracy: 0.8968\n\nEpoch 00025: val_auc did not improve from 0.97032\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0897 - auc: 0.9950 - binary_accuracy: 0.9672 - val_loss: 0.3693 - val_auc: 0.9668 - val_binary_accuracy: 0.8629\n\nEpoch 00026: val_auc did not improve from 0.97032\n\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0874 - auc: 0.9957 - binary_accuracy: 0.9673 - val_loss: 0.2612 - val_auc: 0.9730 - val_binary_accuracy: 0.8943\n\nEpoch 00027: val_auc improved from 0.97032 to 0.97302, saving model to Fold_1_2_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0899 - auc: 0.9954 - binary_accuracy: 0.9657 - val_loss: 0.2767 - val_auc: 0.9707 - val_binary_accuracy: 0.8885\n\nEpoch 00028: val_auc did not improve from 0.97302\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1028 - auc: 0.9938 - binary_accuracy: 0.9604 - val_loss: 0.2514 - val_auc: 0.9712 - val_binary_accuracy: 0.8939\n\nEpoch 00029: val_auc did not improve from 0.97302\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1115 - auc: 0.9927 - binary_accuracy: 0.9576 - val_loss: 0.2295 - val_auc: 0.9739 - val_binary_accuracy: 0.9103\n\nEpoch 00030: val_auc improved from 0.97302 to 0.97386, saving model to Fold_1_2_weights.h5\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1168 - auc: 0.9919 - binary_accuracy: 0.9549 - val_loss: 0.1856 - val_auc: 0.9800 - val_binary_accuracy: 0.9257\n\nEpoch 00031: val_auc improved from 0.97386 to 0.98005, saving model to Fold_1_2_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1159 - auc: 0.9920 - binary_accuracy: 0.9540 - val_loss: 0.1936 - val_auc: 0.9781 - val_binary_accuracy: 0.9257\n\nEpoch 00032: val_auc did not improve from 0.98005\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1338 - auc: 0.9892 - binary_accuracy: 0.9487 - val_loss: 0.1862 - val_auc: 0.9796 - val_binary_accuracy: 0.9255\n\nEpoch 00033: val_auc did not improve from 0.98005\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1329 - auc: 0.9894 - binary_accuracy: 0.9488 - val_loss: 0.2185 - val_auc: 0.9805 - val_binary_accuracy: 0.9180\n\nEpoch 00034: val_auc improved from 0.98005 to 0.98046, saving model to Fold_1_2_weights.h5\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1484 - auc: 0.9871 - binary_accuracy: 0.9407 - val_loss: 0.1911 - val_auc: 0.9796 - val_binary_accuracy: 0.9236\n\nEpoch 00035: val_auc did not improve from 0.98046\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1348 - auc: 0.9891 - binary_accuracy: 0.9459 - val_loss: 0.2015 - val_auc: 0.9781 - val_binary_accuracy: 0.9191\n\nEpoch 00036: val_auc did not improve from 0.98046\nEpoch 37/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1398 - auc: 0.9884 - binary_accuracy: 0.9461 - val_loss: 0.1981 - val_auc: 0.9799 - val_binary_accuracy: 0.9211\n\nEpoch 00037: val_auc did not improve from 0.98046\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1505 - auc: 0.9861 - binary_accuracy: 0.9418 - val_loss: 0.1894 - val_auc: 0.9800 - val_binary_accuracy: 0.9228\n\nEpoch 00038: val_auc did not improve from 0.98046\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1441 - auc: 0.9876 - binary_accuracy: 0.9447 - val_loss: 0.1887 - val_auc: 0.9797 - val_binary_accuracy: 0.9243\n\nEpoch 00039: val_auc did not improve from 0.98046\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1667 - auc: 0.9836 - binary_accuracy: 0.9352 - val_loss: 0.2048 - val_auc: 0.9772 - val_binary_accuracy: 0.9172\n\nEpoch 00040: val_auc did not improve from 0.98046\n****************************** Fold 1 Trial 3 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 304ms/step - loss: 0.6322 - auc: 0.7297 - binary_accuracy: 0.6746 - val_loss: 0.6921 - val_auc: 0.5906 - val_binary_accuracy: 0.4950\n\nEpoch 00001: val_auc improved from -inf to 0.59055, saving model to Fold_1_3_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3189 - auc: 0.9364 - binary_accuracy: 0.8634 - val_loss: 1.2414 - val_auc: 0.6522 - val_binary_accuracy: 0.5033\n\nEpoch 00002: val_auc improved from 0.59055 to 0.65221, saving model to Fold_1_3_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2624 - auc: 0.9583 - binary_accuracy: 0.8941 - val_loss: 1.2913 - val_auc: 0.6311 - val_binary_accuracy: 0.5031\n\nEpoch 00003: val_auc did not improve from 0.65221\nEpoch 4/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2251 - auc: 0.9688 - binary_accuracy: 0.9068 - val_loss: 1.2249 - val_auc: 0.6654 - val_binary_accuracy: 0.5071\n\nEpoch 00004: val_auc improved from 0.65221 to 0.66535, saving model to Fold_1_3_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2395 - auc: 0.9652 - binary_accuracy: 0.9017 - val_loss: 1.0984 - val_auc: 0.7756 - val_binary_accuracy: 0.5322\n\nEpoch 00005: val_auc improved from 0.66535 to 0.77562, saving model to Fold_1_3_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1796 - auc: 0.9803 - binary_accuracy: 0.9260 - val_loss: 0.8145 - val_auc: 0.8397 - val_binary_accuracy: 0.6099\n\nEpoch 00006: val_auc improved from 0.77562 to 0.83973, saving model to Fold_1_3_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1822 - auc: 0.9799 - binary_accuracy: 0.9250 - val_loss: 0.6980 - val_auc: 0.9061 - val_binary_accuracy: 0.6923\n\nEpoch 00007: val_auc improved from 0.83973 to 0.90608, saving model to Fold_1_3_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1796 - auc: 0.9806 - binary_accuracy: 0.9285 - val_loss: 0.2994 - val_auc: 0.9506 - val_binary_accuracy: 0.8804\n\nEpoch 00008: val_auc improved from 0.90608 to 0.95061, saving model to Fold_1_3_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1678 - auc: 0.9826 - binary_accuracy: 0.9333 - val_loss: 0.3870 - val_auc: 0.9597 - val_binary_accuracy: 0.8471\n\nEpoch 00009: val_auc improved from 0.95061 to 0.95972, saving model to Fold_1_3_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1560 - auc: 0.9853 - binary_accuracy: 0.9350 - val_loss: 0.2322 - val_auc: 0.9709 - val_binary_accuracy: 0.9057\n\nEpoch 00010: val_auc improved from 0.95972 to 0.97094, saving model to Fold_1_3_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 76ms/step - loss: 0.1481 - auc: 0.9865 - binary_accuracy: 0.9441 - val_loss: 0.3246 - val_auc: 0.9514 - val_binary_accuracy: 0.8650\n\nEpoch 00011: val_auc did not improve from 0.97094\nEpoch 12/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.1482 - auc: 0.9864 - binary_accuracy: 0.9414 - val_loss: 0.3261 - val_auc: 0.9675 - val_binary_accuracy: 0.8776\n\nEpoch 00012: val_auc did not improve from 0.97094\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1411 - auc: 0.9880 - binary_accuracy: 0.9417 - val_loss: 0.3346 - val_auc: 0.9636 - val_binary_accuracy: 0.8864\n\nEpoch 00013: val_auc did not improve from 0.97094\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1402 - auc: 0.9879 - binary_accuracy: 0.9453 - val_loss: 0.2067 - val_auc: 0.9765 - val_binary_accuracy: 0.9134\n\nEpoch 00014: val_auc improved from 0.97094 to 0.97649, saving model to Fold_1_3_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1323 - auc: 0.9893 - binary_accuracy: 0.9465 - val_loss: 0.2276 - val_auc: 0.9750 - val_binary_accuracy: 0.9157\n\nEpoch 00015: val_auc did not improve from 0.97649\nEpoch 16/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1274 - auc: 0.9900 - binary_accuracy: 0.9509 - val_loss: 0.2467 - val_auc: 0.9745 - val_binary_accuracy: 0.9080\n\nEpoch 00016: val_auc did not improve from 0.97649\nEpoch 17/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1357 - auc: 0.9885 - binary_accuracy: 0.9489 - val_loss: 0.2490 - val_auc: 0.9735 - val_binary_accuracy: 0.9033\n\nEpoch 00017: val_auc did not improve from 0.97649\nEpoch 18/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1262 - auc: 0.9903 - binary_accuracy: 0.9483 - val_loss: 0.3445 - val_auc: 0.9674 - val_binary_accuracy: 0.8710\n\nEpoch 00018: val_auc did not improve from 0.97649\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1211 - auc: 0.9911 - binary_accuracy: 0.9531 - val_loss: 0.2498 - val_auc: 0.9698 - val_binary_accuracy: 0.9047\n\nEpoch 00019: val_auc did not improve from 0.97649\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0936 - auc: 0.9945 - binary_accuracy: 0.9640 - val_loss: 0.2379 - val_auc: 0.9744 - val_binary_accuracy: 0.9112\n\nEpoch 00020: val_auc did not improve from 0.97649\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0939 - auc: 0.9944 - binary_accuracy: 0.9642 - val_loss: 0.2725 - val_auc: 0.9678 - val_binary_accuracy: 0.8926\n\nEpoch 00021: val_auc did not improve from 0.97649\nEpoch 22/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0939 - auc: 0.9945 - binary_accuracy: 0.9643 - val_loss: 0.2716 - val_auc: 0.9732 - val_binary_accuracy: 0.8928\n\nEpoch 00022: val_auc did not improve from 0.97649\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0813 - auc: 0.9961 - binary_accuracy: 0.9692 - val_loss: 0.2799 - val_auc: 0.9693 - val_binary_accuracy: 0.8978\n\nEpoch 00023: val_auc did not improve from 0.97649\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0828 - auc: 0.9956 - binary_accuracy: 0.9703 - val_loss: 0.2410 - val_auc: 0.9727 - val_binary_accuracy: 0.9014\n\nEpoch 00024: val_auc did not improve from 0.97649\nEpoch 25/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0830 - auc: 0.9957 - binary_accuracy: 0.9706 - val_loss: 0.2258 - val_auc: 0.9747 - val_binary_accuracy: 0.9130\n\nEpoch 00025: val_auc did not improve from 0.97649\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0861 - auc: 0.9955 - binary_accuracy: 0.9680 - val_loss: 0.2145 - val_auc: 0.9759 - val_binary_accuracy: 0.9164\n\nEpoch 00026: val_auc did not improve from 0.97649\n\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0866 - auc: 0.9955 - binary_accuracy: 0.9664 - val_loss: 0.2035 - val_auc: 0.9775 - val_binary_accuracy: 0.9180\n\nEpoch 00027: val_auc improved from 0.97649 to 0.97746, saving model to Fold_1_3_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0851 - auc: 0.9957 - binary_accuracy: 0.9676 - val_loss: 0.2009 - val_auc: 0.9768 - val_binary_accuracy: 0.9188\n\nEpoch 00028: val_auc did not improve from 0.97746\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0976 - auc: 0.9946 - binary_accuracy: 0.9610 - val_loss: 0.2191 - val_auc: 0.9795 - val_binary_accuracy: 0.9136\n\nEpoch 00029: val_auc improved from 0.97746 to 0.97953, saving model to Fold_1_3_weights.h5\nEpoch 30/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1093 - auc: 0.9931 - binary_accuracy: 0.9569 - val_loss: 0.1926 - val_auc: 0.9794 - val_binary_accuracy: 0.9241\n\nEpoch 00030: val_auc did not improve from 0.97953\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1124 - auc: 0.9922 - binary_accuracy: 0.9576 - val_loss: 0.1945 - val_auc: 0.9799 - val_binary_accuracy: 0.9241\n\nEpoch 00031: val_auc improved from 0.97953 to 0.97987, saving model to Fold_1_3_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1144 - auc: 0.9922 - binary_accuracy: 0.9542 - val_loss: 0.1803 - val_auc: 0.9814 - val_binary_accuracy: 0.9240\n\nEpoch 00032: val_auc improved from 0.97987 to 0.98135, saving model to Fold_1_3_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1212 - auc: 0.9915 - binary_accuracy: 0.9524 - val_loss: 0.1762 - val_auc: 0.9819 - val_binary_accuracy: 0.9293\n\nEpoch 00033: val_auc improved from 0.98135 to 0.98190, saving model to Fold_1_3_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1188 - auc: 0.9916 - binary_accuracy: 0.9536 - val_loss: 0.1726 - val_auc: 0.9823 - val_binary_accuracy: 0.9303\n\nEpoch 00034: val_auc improved from 0.98190 to 0.98234, saving model to Fold_1_3_weights.h5\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1372 - auc: 0.9886 - binary_accuracy: 0.9441 - val_loss: 0.1830 - val_auc: 0.9811 - val_binary_accuracy: 0.9255\n\nEpoch 00035: val_auc did not improve from 0.98234\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1293 - auc: 0.9899 - binary_accuracy: 0.9502 - val_loss: 0.1816 - val_auc: 0.9816 - val_binary_accuracy: 0.9251\n\nEpoch 00036: val_auc did not improve from 0.98234\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1453 - auc: 0.9873 - binary_accuracy: 0.9420 - val_loss: 0.1845 - val_auc: 0.9806 - val_binary_accuracy: 0.9243\n\nEpoch 00037: val_auc did not improve from 0.98234\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1460 - auc: 0.9873 - binary_accuracy: 0.9417 - val_loss: 0.1843 - val_auc: 0.9803 - val_binary_accuracy: 0.9255\n\nEpoch 00038: val_auc did not improve from 0.98234\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1440 - auc: 0.9879 - binary_accuracy: 0.9434 - val_loss: 0.1842 - val_auc: 0.9801 - val_binary_accuracy: 0.9263\n\nEpoch 00039: val_auc did not improve from 0.98234\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1546 - auc: 0.9859 - binary_accuracy: 0.9393 - val_loss: 0.1882 - val_auc: 0.9794 - val_binary_accuracy: 0.9234\n\nEpoch 00040: val_auc did not improve from 0.98234\n****************************** Fold 2 Trial 1 ******************************\nEpoch 1/40\n82/82 [==============================] - 53s 298ms/step - loss: 0.6204 - auc: 0.7431 - binary_accuracy: 0.6893 - val_loss: 0.6912 - val_auc: 0.5608 - val_binary_accuracy: 0.5529\n\nEpoch 00001: val_auc improved from -inf to 0.56076, saving model to Fold_2_1_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3442 - auc: 0.9259 - binary_accuracy: 0.8519 - val_loss: 1.6469 - val_auc: 0.6110 - val_binary_accuracy: 0.4992\n\nEpoch 00002: val_auc improved from 0.56076 to 0.61097, saving model to Fold_2_1_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2416 - auc: 0.9642 - binary_accuracy: 0.9014 - val_loss: 1.3058 - val_auc: 0.5851 - val_binary_accuracy: 0.4994\n\nEpoch 00003: val_auc did not improve from 0.61097\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2147 - auc: 0.9718 - binary_accuracy: 0.9158 - val_loss: 1.3180 - val_auc: 0.6622 - val_binary_accuracy: 0.5023\n\nEpoch 00004: val_auc improved from 0.61097 to 0.66216, saving model to Fold_2_1_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2035 - auc: 0.9748 - binary_accuracy: 0.9161 - val_loss: 1.5428 - val_auc: 0.7448 - val_binary_accuracy: 0.5129\n\nEpoch 00005: val_auc improved from 0.66216 to 0.74481, saving model to Fold_2_1_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2040 - auc: 0.9748 - binary_accuracy: 0.9167 - val_loss: 1.2179 - val_auc: 0.7949 - val_binary_accuracy: 0.5331\n\nEpoch 00006: val_auc improved from 0.74481 to 0.79489, saving model to Fold_2_1_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1762 - auc: 0.9809 - binary_accuracy: 0.9302 - val_loss: 0.8124 - val_auc: 0.8995 - val_binary_accuracy: 0.6207\n\nEpoch 00007: val_auc improved from 0.79489 to 0.89946, saving model to Fold_2_1_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1694 - auc: 0.9826 - binary_accuracy: 0.9326 - val_loss: 0.4548 - val_auc: 0.9487 - val_binary_accuracy: 0.8021\n\nEpoch 00008: val_auc improved from 0.89946 to 0.94867, saving model to Fold_2_1_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1675 - auc: 0.9826 - binary_accuracy: 0.9341 - val_loss: 0.2991 - val_auc: 0.9610 - val_binary_accuracy: 0.8760\n\nEpoch 00009: val_auc improved from 0.94867 to 0.96096, saving model to Fold_2_1_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1651 - auc: 0.9834 - binary_accuracy: 0.9329 - val_loss: 0.2560 - val_auc: 0.9670 - val_binary_accuracy: 0.8933\n\nEpoch 00010: val_auc improved from 0.96096 to 0.96705, saving model to Fold_2_1_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1459 - auc: 0.9870 - binary_accuracy: 0.9421 - val_loss: 0.2904 - val_auc: 0.9675 - val_binary_accuracy: 0.8777\n\nEpoch 00011: val_auc improved from 0.96705 to 0.96750, saving model to Fold_2_1_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1500 - auc: 0.9860 - binary_accuracy: 0.9408 - val_loss: 0.3062 - val_auc: 0.9635 - val_binary_accuracy: 0.8731\n\nEpoch 00012: val_auc did not improve from 0.96750\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1545 - auc: 0.9855 - binary_accuracy: 0.9357 - val_loss: 0.2753 - val_auc: 0.9702 - val_binary_accuracy: 0.8858\n\nEpoch 00013: val_auc improved from 0.96750 to 0.97016, saving model to Fold_2_1_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1362 - auc: 0.9885 - binary_accuracy: 0.9475 - val_loss: 0.3409 - val_auc: 0.9693 - val_binary_accuracy: 0.8660\n\nEpoch 00014: val_auc did not improve from 0.97016\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1400 - auc: 0.9878 - binary_accuracy: 0.9453 - val_loss: 0.2287 - val_auc: 0.9722 - val_binary_accuracy: 0.9089\n\nEpoch 00015: val_auc improved from 0.97016 to 0.97222, saving model to Fold_2_1_weights.h5\nEpoch 16/40\n82/82 [==============================] - 6s 77ms/step - loss: 0.1517 - auc: 0.9861 - binary_accuracy: 0.9404 - val_loss: 0.2746 - val_auc: 0.9723 - val_binary_accuracy: 0.8983\n\nEpoch 00016: val_auc improved from 0.97222 to 0.97232, saving model to Fold_2_1_weights.h5\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1272 - auc: 0.9899 - binary_accuracy: 0.9494 - val_loss: 0.2716 - val_auc: 0.9728 - val_binary_accuracy: 0.8964\n\nEpoch 00017: val_auc improved from 0.97232 to 0.97282, saving model to Fold_2_1_weights.h5\nEpoch 18/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1330 - auc: 0.9889 - binary_accuracy: 0.9474 - val_loss: 0.2326 - val_auc: 0.9712 - val_binary_accuracy: 0.9078\n\nEpoch 00018: val_auc did not improve from 0.97282\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1394 - auc: 0.9878 - binary_accuracy: 0.9448 - val_loss: 0.2875 - val_auc: 0.9724 - val_binary_accuracy: 0.8885\n\nEpoch 00019: val_auc did not improve from 0.97282\nEpoch 20/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1249 - auc: 0.9904 - binary_accuracy: 0.9500 - val_loss: 0.2420 - val_auc: 0.9695 - val_binary_accuracy: 0.9024\n\nEpoch 00020: val_auc did not improve from 0.97282\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1278 - auc: 0.9897 - binary_accuracy: 0.9500 - val_loss: 0.2372 - val_auc: 0.9749 - val_binary_accuracy: 0.9055\n\nEpoch 00021: val_auc improved from 0.97282 to 0.97492, saving model to Fold_2_1_weights.h5\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1221 - auc: 0.9906 - binary_accuracy: 0.9533 - val_loss: 0.2273 - val_auc: 0.9728 - val_binary_accuracy: 0.9130\n\nEpoch 00022: val_auc did not improve from 0.97492\nEpoch 23/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1163 - auc: 0.9916 - binary_accuracy: 0.9533 - val_loss: 0.2306 - val_auc: 0.9737 - val_binary_accuracy: 0.9145\n\nEpoch 00023: val_auc did not improve from 0.97492\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1212 - auc: 0.9908 - binary_accuracy: 0.9531 - val_loss: 0.2458 - val_auc: 0.9704 - val_binary_accuracy: 0.9105\n\nEpoch 00024: val_auc did not improve from 0.97492\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1321 - auc: 0.9892 - binary_accuracy: 0.9478 - val_loss: 0.2363 - val_auc: 0.9738 - val_binary_accuracy: 0.9139\n\nEpoch 00025: val_auc did not improve from 0.97492\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1077 - auc: 0.9932 - binary_accuracy: 0.9600 - val_loss: 0.2632 - val_auc: 0.9745 - val_binary_accuracy: 0.9057\n\nEpoch 00026: val_auc did not improve from 0.97492\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0870 - auc: 0.9951 - binary_accuracy: 0.9661 - val_loss: 0.2726 - val_auc: 0.9708 - val_binary_accuracy: 0.9008\n\nEpoch 00027: val_auc did not improve from 0.97492\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0901 - auc: 0.9948 - binary_accuracy: 0.9648 - val_loss: 0.2605 - val_auc: 0.9710 - val_binary_accuracy: 0.9062\n\nEpoch 00028: val_auc did not improve from 0.97492\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0864 - auc: 0.9953 - binary_accuracy: 0.9671 - val_loss: 0.2336 - val_auc: 0.9740 - val_binary_accuracy: 0.9124\n\nEpoch 00029: val_auc did not improve from 0.97492\n\nEpoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0799 - auc: 0.9960 - binary_accuracy: 0.9708 - val_loss: 0.3207 - val_auc: 0.9700 - val_binary_accuracy: 0.8889\n\nEpoch 00030: val_auc did not improve from 0.97492\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0743 - auc: 0.9966 - binary_accuracy: 0.9739 - val_loss: 0.2423 - val_auc: 0.9734 - val_binary_accuracy: 0.9103\n\nEpoch 00031: val_auc did not improve from 0.97492\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0793 - auc: 0.9960 - binary_accuracy: 0.9696 - val_loss: 0.2110 - val_auc: 0.9773 - val_binary_accuracy: 0.9209\n\nEpoch 00032: val_auc improved from 0.97492 to 0.97727, saving model to Fold_2_1_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0790 - auc: 0.9960 - binary_accuracy: 0.9704 - val_loss: 0.1875 - val_auc: 0.9802 - val_binary_accuracy: 0.9268\n\nEpoch 00033: val_auc improved from 0.97727 to 0.98022, saving model to Fold_2_1_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0874 - auc: 0.9955 - binary_accuracy: 0.9681 - val_loss: 0.1850 - val_auc: 0.9798 - val_binary_accuracy: 0.9290\n\nEpoch 00034: val_auc did not improve from 0.98022\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0886 - auc: 0.9953 - binary_accuracy: 0.9653 - val_loss: 0.1863 - val_auc: 0.9802 - val_binary_accuracy: 0.9305\n\nEpoch 00035: val_auc improved from 0.98022 to 0.98024, saving model to Fold_2_1_weights.h5\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0892 - auc: 0.9952 - binary_accuracy: 0.9676 - val_loss: 0.1805 - val_auc: 0.9805 - val_binary_accuracy: 0.9313\n\nEpoch 00036: val_auc improved from 0.98024 to 0.98046, saving model to Fold_2_1_weights.h5\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0999 - auc: 0.9941 - binary_accuracy: 0.9605 - val_loss: 0.1811 - val_auc: 0.9812 - val_binary_accuracy: 0.9291\n\nEpoch 00037: val_auc improved from 0.98046 to 0.98116, saving model to Fold_2_1_weights.h5\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0931 - auc: 0.9948 - binary_accuracy: 0.9647 - val_loss: 0.2427 - val_auc: 0.9775 - val_binary_accuracy: 0.9118\n\nEpoch 00038: val_auc did not improve from 0.98116\nEpoch 39/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1128 - auc: 0.9923 - binary_accuracy: 0.9548 - val_loss: 0.1928 - val_auc: 0.9799 - val_binary_accuracy: 0.9249\n\nEpoch 00039: val_auc did not improve from 0.98116\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1050 - auc: 0.9935 - binary_accuracy: 0.9604 - val_loss: 0.1778 - val_auc: 0.9819 - val_binary_accuracy: 0.9278\n\nEpoch 00040: val_auc improved from 0.98116 to 0.98191, saving model to Fold_2_1_weights.h5\n****************************** Fold 2 Trial 2 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 299ms/step - loss: 0.6389 - auc: 0.7501 - binary_accuracy: 0.6903 - val_loss: 1.1720 - val_auc: 0.7432 - val_binary_accuracy: 0.5198\n\nEpoch 00001: val_auc improved from -inf to 0.74318, saving model to Fold_2_2_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2891 - auc: 0.9487 - binary_accuracy: 0.8775 - val_loss: 0.8865 - val_auc: 0.7649 - val_binary_accuracy: 0.5547\n\nEpoch 00002: val_auc improved from 0.74318 to 0.76494, saving model to Fold_2_2_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.2141 - auc: 0.9719 - binary_accuracy: 0.9123 - val_loss: 0.7620 - val_auc: 0.8521 - val_binary_accuracy: 0.6117\n\nEpoch 00003: val_auc improved from 0.76494 to 0.85205, saving model to Fold_2_2_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1840 - auc: 0.9794 - binary_accuracy: 0.9261 - val_loss: 1.0437 - val_auc: 0.8502 - val_binary_accuracy: 0.5757\n\nEpoch 00004: val_auc did not improve from 0.85205\nEpoch 5/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1912 - auc: 0.9782 - binary_accuracy: 0.9240 - val_loss: 0.7669 - val_auc: 0.8979 - val_binary_accuracy: 0.6821\n\nEpoch 00005: val_auc improved from 0.85205 to 0.89788, saving model to Fold_2_2_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1494 - auc: 0.9862 - binary_accuracy: 0.9432 - val_loss: 0.4453 - val_auc: 0.9377 - val_binary_accuracy: 0.7909\n\nEpoch 00006: val_auc improved from 0.89788 to 0.93774, saving model to Fold_2_2_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1665 - auc: 0.9832 - binary_accuracy: 0.9336 - val_loss: 0.3541 - val_auc: 0.9535 - val_binary_accuracy: 0.8608\n\nEpoch 00007: val_auc improved from 0.93774 to 0.95346, saving model to Fold_2_2_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1471 - auc: 0.9869 - binary_accuracy: 0.9419 - val_loss: 0.2545 - val_auc: 0.9667 - val_binary_accuracy: 0.8958\n\nEpoch 00008: val_auc improved from 0.95346 to 0.96668, saving model to Fold_2_2_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1349 - auc: 0.9892 - binary_accuracy: 0.9483 - val_loss: 0.2644 - val_auc: 0.9685 - val_binary_accuracy: 0.8980\n\nEpoch 00009: val_auc improved from 0.96668 to 0.96847, saving model to Fold_2_2_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1286 - auc: 0.9897 - binary_accuracy: 0.9502 - val_loss: 0.3235 - val_auc: 0.9686 - val_binary_accuracy: 0.8812\n\nEpoch 00010: val_auc improved from 0.96847 to 0.96863, saving model to Fold_2_2_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1310 - auc: 0.9896 - binary_accuracy: 0.9488 - val_loss: 0.2487 - val_auc: 0.9728 - val_binary_accuracy: 0.9080\n\nEpoch 00011: val_auc improved from 0.96863 to 0.97278, saving model to Fold_2_2_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1181 - auc: 0.9913 - binary_accuracy: 0.9542 - val_loss: 0.2616 - val_auc: 0.9724 - val_binary_accuracy: 0.9003\n\nEpoch 00012: val_auc did not improve from 0.97278\nEpoch 13/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1232 - auc: 0.9905 - binary_accuracy: 0.9522 - val_loss: 0.2648 - val_auc: 0.9673 - val_binary_accuracy: 0.8999\n\nEpoch 00013: val_auc did not improve from 0.97278\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1073 - auc: 0.9929 - binary_accuracy: 0.9574 - val_loss: 0.2008 - val_auc: 0.9775 - val_binary_accuracy: 0.9184\n\nEpoch 00014: val_auc improved from 0.97278 to 0.97747, saving model to Fold_2_2_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1119 - auc: 0.9923 - binary_accuracy: 0.9581 - val_loss: 0.1846 - val_auc: 0.9806 - val_binary_accuracy: 0.9274\n\nEpoch 00015: val_auc improved from 0.97747 to 0.98056, saving model to Fold_2_2_weights.h5\nEpoch 16/40\n82/82 [==============================] - 6s 72ms/step - loss: 0.1138 - auc: 0.9914 - binary_accuracy: 0.9576 - val_loss: 0.2421 - val_auc: 0.9714 - val_binary_accuracy: 0.9130\n\nEpoch 00016: val_auc did not improve from 0.98056\nEpoch 17/40\n82/82 [==============================] - 6s 74ms/step - loss: 0.1337 - auc: 0.9894 - binary_accuracy: 0.9471 - val_loss: 0.1997 - val_auc: 0.9776 - val_binary_accuracy: 0.9234\n\nEpoch 00017: val_auc did not improve from 0.98056\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0923 - auc: 0.9945 - binary_accuracy: 0.9648 - val_loss: 0.2687 - val_auc: 0.9775 - val_binary_accuracy: 0.9026\n\nEpoch 00018: val_auc did not improve from 0.98056\nEpoch 19/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0995 - auc: 0.9938 - binary_accuracy: 0.9608 - val_loss: 0.2652 - val_auc: 0.9764 - val_binary_accuracy: 0.9068\n\nEpoch 00019: val_auc did not improve from 0.98056\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0828 - auc: 0.9955 - binary_accuracy: 0.9700 - val_loss: 0.2329 - val_auc: 0.9773 - val_binary_accuracy: 0.9174\n\nEpoch 00020: val_auc did not improve from 0.98056\nEpoch 21/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0661 - auc: 0.9969 - binary_accuracy: 0.9763 - val_loss: 0.2580 - val_auc: 0.9744 - val_binary_accuracy: 0.9118\n\nEpoch 00021: val_auc did not improve from 0.98056\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0649 - auc: 0.9972 - binary_accuracy: 0.9775 - val_loss: 0.2020 - val_auc: 0.9793 - val_binary_accuracy: 0.9290\n\nEpoch 00022: val_auc did not improve from 0.98056\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0621 - auc: 0.9975 - binary_accuracy: 0.9773 - val_loss: 0.2250 - val_auc: 0.9761 - val_binary_accuracy: 0.9226\n\nEpoch 00023: val_auc did not improve from 0.98056\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0519 - auc: 0.9980 - binary_accuracy: 0.9840 - val_loss: 0.2275 - val_auc: 0.9766 - val_binary_accuracy: 0.9218\n\nEpoch 00024: val_auc did not improve from 0.98056\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0455 - auc: 0.9988 - binary_accuracy: 0.9849 - val_loss: 0.2158 - val_auc: 0.9779 - val_binary_accuracy: 0.9226\n\nEpoch 00025: val_auc did not improve from 0.98056\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0559 - auc: 0.9980 - binary_accuracy: 0.9812 - val_loss: 0.2204 - val_auc: 0.9763 - val_binary_accuracy: 0.9222\n\nEpoch 00026: val_auc did not improve from 0.98056\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0580 - auc: 0.9978 - binary_accuracy: 0.9794 - val_loss: 0.1942 - val_auc: 0.9803 - val_binary_accuracy: 0.9284\n\nEpoch 00027: val_auc did not improve from 0.98056\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0530 - auc: 0.9984 - binary_accuracy: 0.9818 - val_loss: 0.1832 - val_auc: 0.9820 - val_binary_accuracy: 0.9324\n\nEpoch 00028: val_auc improved from 0.98056 to 0.98202, saving model to Fold_2_2_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 78ms/step - loss: 0.0559 - auc: 0.9982 - binary_accuracy: 0.9817 - val_loss: 0.1764 - val_auc: 0.9825 - val_binary_accuracy: 0.9345\n\nEpoch 00029: val_auc improved from 0.98202 to 0.98254, saving model to Fold_2_2_weights.h5\nEpoch 30/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0640 - auc: 0.9977 - binary_accuracy: 0.9775 - val_loss: 0.1887 - val_auc: 0.9805 - val_binary_accuracy: 0.9270\n\nEpoch 00030: val_auc did not improve from 0.98254\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0731 - auc: 0.9969 - binary_accuracy: 0.9727 - val_loss: 0.1791 - val_auc: 0.9826 - val_binary_accuracy: 0.9334\n\nEpoch 00031: val_auc improved from 0.98254 to 0.98260, saving model to Fold_2_2_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0820 - auc: 0.9961 - binary_accuracy: 0.9695 - val_loss: 0.1746 - val_auc: 0.9833 - val_binary_accuracy: 0.9293\n\nEpoch 00032: val_auc improved from 0.98260 to 0.98335, saving model to Fold_2_2_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0877 - auc: 0.9954 - binary_accuracy: 0.9689 - val_loss: 0.1630 - val_auc: 0.9845 - val_binary_accuracy: 0.9367\n\nEpoch 00033: val_auc improved from 0.98335 to 0.98453, saving model to Fold_2_2_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0964 - auc: 0.9945 - binary_accuracy: 0.9621 - val_loss: 0.1629 - val_auc: 0.9842 - val_binary_accuracy: 0.9369\n\nEpoch 00034: val_auc did not improve from 0.98453\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0980 - auc: 0.9944 - binary_accuracy: 0.9613 - val_loss: 0.1785 - val_auc: 0.9822 - val_binary_accuracy: 0.9295\n\nEpoch 00035: val_auc did not improve from 0.98453\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1125 - auc: 0.9925 - binary_accuracy: 0.9579 - val_loss: 0.1802 - val_auc: 0.9836 - val_binary_accuracy: 0.9318\n\nEpoch 00036: val_auc did not improve from 0.98453\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1188 - auc: 0.9915 - binary_accuracy: 0.9524 - val_loss: 0.1625 - val_auc: 0.9851 - val_binary_accuracy: 0.9409\n\nEpoch 00037: val_auc improved from 0.98453 to 0.98514, saving model to Fold_2_2_weights.h5\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1208 - auc: 0.9913 - binary_accuracy: 0.9541 - val_loss: 0.1650 - val_auc: 0.9845 - val_binary_accuracy: 0.9376\n\nEpoch 00038: val_auc did not improve from 0.98514\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1211 - auc: 0.9911 - binary_accuracy: 0.9525 - val_loss: 0.1704 - val_auc: 0.9832 - val_binary_accuracy: 0.9343\n\nEpoch 00039: val_auc did not improve from 0.98514\nEpoch 40/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1218 - auc: 0.9911 - binary_accuracy: 0.9519 - val_loss: 0.1691 - val_auc: 0.9834 - val_binary_accuracy: 0.9318\n\nEpoch 00040: val_auc did not improve from 0.98514\n****************************** Fold 2 Trial 3 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 297ms/step - loss: 0.7133 - auc: 0.6940 - binary_accuracy: 0.6428 - val_loss: 1.0090 - val_auc: 0.7787 - val_binary_accuracy: 0.5169\n\nEpoch 00001: val_auc improved from -inf to 0.77868, saving model to Fold_2_3_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3142 - auc: 0.9396 - binary_accuracy: 0.8639 - val_loss: 1.1368 - val_auc: 0.8679 - val_binary_accuracy: 0.5356\n\nEpoch 00002: val_auc improved from 0.77868 to 0.86789, saving model to Fold_2_3_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2264 - auc: 0.9689 - binary_accuracy: 0.9076 - val_loss: 1.0619 - val_auc: 0.8914 - val_binary_accuracy: 0.5674\n\nEpoch 00003: val_auc improved from 0.86789 to 0.89141, saving model to Fold_2_3_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1884 - auc: 0.9788 - binary_accuracy: 0.9213 - val_loss: 1.0247 - val_auc: 0.9135 - val_binary_accuracy: 0.6017\n\nEpoch 00004: val_auc improved from 0.89141 to 0.91348, saving model to Fold_2_3_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1820 - auc: 0.9798 - binary_accuracy: 0.9264 - val_loss: 0.4915 - val_auc: 0.9439 - val_binary_accuracy: 0.7790\n\nEpoch 00005: val_auc improved from 0.91348 to 0.94392, saving model to Fold_2_3_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1727 - auc: 0.9820 - binary_accuracy: 0.9290 - val_loss: 0.4325 - val_auc: 0.9444 - val_binary_accuracy: 0.8204\n\nEpoch 00006: val_auc improved from 0.94392 to 0.94436, saving model to Fold_2_3_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1551 - auc: 0.9854 - binary_accuracy: 0.9380 - val_loss: 0.4179 - val_auc: 0.9519 - val_binary_accuracy: 0.8329\n\nEpoch 00007: val_auc improved from 0.94436 to 0.95191, saving model to Fold_2_3_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1490 - auc: 0.9864 - binary_accuracy: 0.9395 - val_loss: 0.2939 - val_auc: 0.9652 - val_binary_accuracy: 0.8851\n\nEpoch 00008: val_auc improved from 0.95191 to 0.96523, saving model to Fold_2_3_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1399 - auc: 0.9879 - binary_accuracy: 0.9451 - val_loss: 0.2708 - val_auc: 0.9674 - val_binary_accuracy: 0.8966\n\nEpoch 00009: val_auc improved from 0.96523 to 0.96743, saving model to Fold_2_3_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1317 - auc: 0.9891 - binary_accuracy: 0.9475 - val_loss: 0.2171 - val_auc: 0.9773 - val_binary_accuracy: 0.9132\n\nEpoch 00010: val_auc improved from 0.96743 to 0.97725, saving model to Fold_2_3_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1312 - auc: 0.9896 - binary_accuracy: 0.9488 - val_loss: 0.2271 - val_auc: 0.9715 - val_binary_accuracy: 0.9126\n\nEpoch 00011: val_auc did not improve from 0.97725\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1262 - auc: 0.9900 - binary_accuracy: 0.9532 - val_loss: 0.2324 - val_auc: 0.9716 - val_binary_accuracy: 0.9076\n\nEpoch 00012: val_auc did not improve from 0.97725\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1344 - auc: 0.9888 - binary_accuracy: 0.9483 - val_loss: 0.2191 - val_auc: 0.9774 - val_binary_accuracy: 0.9153\n\nEpoch 00013: val_auc improved from 0.97725 to 0.97740, saving model to Fold_2_3_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1174 - auc: 0.9915 - binary_accuracy: 0.9531 - val_loss: 0.2630 - val_auc: 0.9707 - val_binary_accuracy: 0.9014\n\nEpoch 00014: val_auc did not improve from 0.97740\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1333 - auc: 0.9891 - binary_accuracy: 0.9464 - val_loss: 0.2661 - val_auc: 0.9702 - val_binary_accuracy: 0.8987\n\nEpoch 00015: val_auc did not improve from 0.97740\nEpoch 16/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1172 - auc: 0.9916 - binary_accuracy: 0.9547 - val_loss: 0.1917 - val_auc: 0.9783 - val_binary_accuracy: 0.9253\n\nEpoch 00016: val_auc improved from 0.97740 to 0.97829, saving model to Fold_2_3_weights.h5\nEpoch 17/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1047 - auc: 0.9931 - binary_accuracy: 0.9584 - val_loss: 0.2071 - val_auc: 0.9746 - val_binary_accuracy: 0.9149\n\nEpoch 00017: val_auc did not improve from 0.97829\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1020 - auc: 0.9936 - binary_accuracy: 0.9628 - val_loss: 0.2326 - val_auc: 0.9721 - val_binary_accuracy: 0.9089\n\nEpoch 00018: val_auc did not improve from 0.97829\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1150 - auc: 0.9917 - binary_accuracy: 0.9563 - val_loss: 0.2169 - val_auc: 0.9771 - val_binary_accuracy: 0.9213\n\nEpoch 00019: val_auc did not improve from 0.97829\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0937 - auc: 0.9943 - binary_accuracy: 0.9644 - val_loss: 0.2023 - val_auc: 0.9767 - val_binary_accuracy: 0.9191\n\nEpoch 00020: val_auc did not improve from 0.97829\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0812 - auc: 0.9959 - binary_accuracy: 0.9707 - val_loss: 0.2272 - val_auc: 0.9766 - val_binary_accuracy: 0.9178\n\nEpoch 00021: val_auc did not improve from 0.97829\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0603 - auc: 0.9976 - binary_accuracy: 0.9784 - val_loss: 0.2210 - val_auc: 0.9774 - val_binary_accuracy: 0.9213\n\nEpoch 00022: val_auc did not improve from 0.97829\nEpoch 23/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0713 - auc: 0.9966 - binary_accuracy: 0.9746 - val_loss: 0.2754 - val_auc: 0.9691 - val_binary_accuracy: 0.9091\n\nEpoch 00023: val_auc did not improve from 0.97829\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0721 - auc: 0.9968 - binary_accuracy: 0.9718 - val_loss: 0.2127 - val_auc: 0.9779 - val_binary_accuracy: 0.9182\n\nEpoch 00024: val_auc did not improve from 0.97829\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0607 - auc: 0.9979 - binary_accuracy: 0.9772 - val_loss: 0.2275 - val_auc: 0.9760 - val_binary_accuracy: 0.9161\n\nEpoch 00025: val_auc did not improve from 0.97829\nEpoch 26/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0517 - auc: 0.9982 - binary_accuracy: 0.9825 - val_loss: 0.2695 - val_auc: 0.9730 - val_binary_accuracy: 0.9066\n\nEpoch 00026: val_auc did not improve from 0.97829\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0612 - auc: 0.9978 - binary_accuracy: 0.9776 - val_loss: 0.2122 - val_auc: 0.9780 - val_binary_accuracy: 0.9255\n\nEpoch 00027: val_auc did not improve from 0.97829\nEpoch 28/40\n82/82 [==============================] - 6s 79ms/step - loss: 0.0780 - auc: 0.9963 - binary_accuracy: 0.9696 - val_loss: 0.1918 - val_auc: 0.9808 - val_binary_accuracy: 0.9276\n\nEpoch 00028: val_auc improved from 0.97829 to 0.98085, saving model to Fold_2_3_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0622 - auc: 0.9977 - binary_accuracy: 0.9774 - val_loss: 0.2060 - val_auc: 0.9782 - val_binary_accuracy: 0.9234\n\nEpoch 00029: val_auc did not improve from 0.98085\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0706 - auc: 0.9970 - binary_accuracy: 0.9744 - val_loss: 0.1960 - val_auc: 0.9798 - val_binary_accuracy: 0.9322\n\nEpoch 00030: val_auc did not improve from 0.98085\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0692 - auc: 0.9969 - binary_accuracy: 0.9753 - val_loss: 0.1840 - val_auc: 0.9817 - val_binary_accuracy: 0.9305\n\nEpoch 00031: val_auc improved from 0.98085 to 0.98171, saving model to Fold_2_3_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0759 - auc: 0.9963 - binary_accuracy: 0.9716 - val_loss: 0.1824 - val_auc: 0.9824 - val_binary_accuracy: 0.9305\n\nEpoch 00032: val_auc improved from 0.98171 to 0.98242, saving model to Fold_2_3_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0775 - auc: 0.9964 - binary_accuracy: 0.9711 - val_loss: 0.1790 - val_auc: 0.9825 - val_binary_accuracy: 0.9313\n\nEpoch 00033: val_auc improved from 0.98242 to 0.98249, saving model to Fold_2_3_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0864 - auc: 0.9956 - binary_accuracy: 0.9657 - val_loss: 0.1736 - val_auc: 0.9834 - val_binary_accuracy: 0.9330\n\nEpoch 00034: val_auc improved from 0.98249 to 0.98343, saving model to Fold_2_3_weights.h5\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0792 - auc: 0.9962 - binary_accuracy: 0.9693 - val_loss: 0.1774 - val_auc: 0.9819 - val_binary_accuracy: 0.9334\n\nEpoch 00035: val_auc did not improve from 0.98343\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0765 - auc: 0.9965 - binary_accuracy: 0.9710 - val_loss: 0.1966 - val_auc: 0.9801 - val_binary_accuracy: 0.9282\n\nEpoch 00036: val_auc did not improve from 0.98343\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0762 - auc: 0.9965 - binary_accuracy: 0.9723 - val_loss: 0.1765 - val_auc: 0.9825 - val_binary_accuracy: 0.9317\n\nEpoch 00037: val_auc did not improve from 0.98343\nEpoch 38/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0822 - auc: 0.9960 - binary_accuracy: 0.9694 - val_loss: 0.1720 - val_auc: 0.9828 - val_binary_accuracy: 0.9282\n\nEpoch 00038: val_auc did not improve from 0.98343\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 39/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0833 - auc: 0.9961 - binary_accuracy: 0.9687 - val_loss: 0.1693 - val_auc: 0.9835 - val_binary_accuracy: 0.9334\n\nEpoch 00039: val_auc improved from 0.98343 to 0.98353, saving model to Fold_2_3_weights.h5\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0854 - auc: 0.9959 - binary_accuracy: 0.9684 - val_loss: 0.1623 - val_auc: 0.9842 - val_binary_accuracy: 0.9336\n\nEpoch 00040: val_auc improved from 0.98353 to 0.98422, saving model to Fold_2_3_weights.h5\n****************************** Fold 3 Trial 1 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 296ms/step - loss: 0.6807 - auc: 0.7522 - binary_accuracy: 0.6898 - val_loss: 1.0534 - val_auc: 0.8702 - val_binary_accuracy: 0.5487\n\nEpoch 00001: val_auc improved from -inf to 0.87020, saving model to Fold_3_1_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2901 - auc: 0.9483 - binary_accuracy: 0.8775 - val_loss: 0.8363 - val_auc: 0.9154 - val_binary_accuracy: 0.5911\n\nEpoch 00002: val_auc improved from 0.87020 to 0.91541, saving model to Fold_3_1_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2093 - auc: 0.9732 - binary_accuracy: 0.9169 - val_loss: 0.6885 - val_auc: 0.9279 - val_binary_accuracy: 0.6492\n\nEpoch 00003: val_auc improved from 0.91541 to 0.92786, saving model to Fold_3_1_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1786 - auc: 0.9806 - binary_accuracy: 0.9267 - val_loss: 1.1197 - val_auc: 0.9041 - val_binary_accuracy: 0.5807\n\nEpoch 00004: val_auc did not improve from 0.92786\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1861 - auc: 0.9790 - binary_accuracy: 0.9242 - val_loss: 0.4869 - val_auc: 0.9419 - val_binary_accuracy: 0.7822\n\nEpoch 00005: val_auc improved from 0.92786 to 0.94193, saving model to Fold_3_1_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1561 - auc: 0.9854 - binary_accuracy: 0.9366 - val_loss: 0.4489 - val_auc: 0.9556 - val_binary_accuracy: 0.8019\n\nEpoch 00006: val_auc improved from 0.94193 to 0.95557, saving model to Fold_3_1_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1644 - auc: 0.9837 - binary_accuracy: 0.9346 - val_loss: 0.3445 - val_auc: 0.9634 - val_binary_accuracy: 0.8641\n\nEpoch 00007: val_auc improved from 0.95557 to 0.96339, saving model to Fold_3_1_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1428 - auc: 0.9874 - binary_accuracy: 0.9438 - val_loss: 0.3369 - val_auc: 0.9773 - val_binary_accuracy: 0.8612\n\nEpoch 00008: val_auc improved from 0.96339 to 0.97728, saving model to Fold_3_1_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1368 - auc: 0.9888 - binary_accuracy: 0.9455 - val_loss: 0.2296 - val_auc: 0.9794 - val_binary_accuracy: 0.9043\n\nEpoch 00009: val_auc improved from 0.97728 to 0.97942, saving model to Fold_3_1_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1326 - auc: 0.9891 - binary_accuracy: 0.9466 - val_loss: 0.3425 - val_auc: 0.9691 - val_binary_accuracy: 0.8801\n\nEpoch 00010: val_auc did not improve from 0.97942\nEpoch 11/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1337 - auc: 0.9891 - binary_accuracy: 0.9489 - val_loss: 0.2135 - val_auc: 0.9755 - val_binary_accuracy: 0.9195\n\nEpoch 00011: val_auc did not improve from 0.97942\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1199 - auc: 0.9912 - binary_accuracy: 0.9520 - val_loss: 0.1746 - val_auc: 0.9822 - val_binary_accuracy: 0.9318\n\nEpoch 00012: val_auc improved from 0.97942 to 0.98222, saving model to Fold_3_1_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1262 - auc: 0.9902 - binary_accuracy: 0.9516 - val_loss: 0.1894 - val_auc: 0.9827 - val_binary_accuracy: 0.9280\n\nEpoch 00013: val_auc improved from 0.98222 to 0.98275, saving model to Fold_3_1_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1087 - auc: 0.9925 - binary_accuracy: 0.9574 - val_loss: 0.2144 - val_auc: 0.9744 - val_binary_accuracy: 0.9186\n\nEpoch 00014: val_auc did not improve from 0.98275\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1187 - auc: 0.9913 - binary_accuracy: 0.9535 - val_loss: 0.1975 - val_auc: 0.9789 - val_binary_accuracy: 0.9251\n\nEpoch 00015: val_auc did not improve from 0.98275\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1108 - auc: 0.9922 - binary_accuracy: 0.9577 - val_loss: 0.2199 - val_auc: 0.9793 - val_binary_accuracy: 0.9172\n\nEpoch 00016: val_auc did not improve from 0.98275\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1093 - auc: 0.9925 - binary_accuracy: 0.9579 - val_loss: 0.2095 - val_auc: 0.9788 - val_binary_accuracy: 0.9230\n\nEpoch 00017: val_auc did not improve from 0.98275\n\nEpoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 18/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0939 - auc: 0.9946 - binary_accuracy: 0.9636 - val_loss: 0.1929 - val_auc: 0.9805 - val_binary_accuracy: 0.9270\n\nEpoch 00018: val_auc did not improve from 0.98275\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0800 - auc: 0.9960 - binary_accuracy: 0.9711 - val_loss: 0.2335 - val_auc: 0.9789 - val_binary_accuracy: 0.9107\n\nEpoch 00019: val_auc did not improve from 0.98275\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0723 - auc: 0.9966 - binary_accuracy: 0.9735 - val_loss: 0.2573 - val_auc: 0.9760 - val_binary_accuracy: 0.9134\n\nEpoch 00020: val_auc did not improve from 0.98275\nEpoch 21/40\n82/82 [==============================] - 7s 81ms/step - loss: 0.0720 - auc: 0.9966 - binary_accuracy: 0.9740 - val_loss: 0.1819 - val_auc: 0.9810 - val_binary_accuracy: 0.9355\n\nEpoch 00021: val_auc did not improve from 0.98275\n\nEpoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 22/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.0657 - auc: 0.9974 - binary_accuracy: 0.9768 - val_loss: 0.2183 - val_auc: 0.9788 - val_binary_accuracy: 0.9193\n\nEpoch 00022: val_auc did not improve from 0.98275\nEpoch 23/40\n82/82 [==============================] - 6s 77ms/step - loss: 0.0605 - auc: 0.9977 - binary_accuracy: 0.9796 - val_loss: 0.2136 - val_auc: 0.9790 - val_binary_accuracy: 0.9213\n\nEpoch 00023: val_auc did not improve from 0.98275\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0695 - auc: 0.9968 - binary_accuracy: 0.9742 - val_loss: 0.2374 - val_auc: 0.9779 - val_binary_accuracy: 0.9122\n\nEpoch 00024: val_auc did not improve from 0.98275\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0830 - auc: 0.9957 - binary_accuracy: 0.9672 - val_loss: 0.1931 - val_auc: 0.9827 - val_binary_accuracy: 0.9278\n\nEpoch 00025: val_auc did not improve from 0.98275\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 26/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0632 - auc: 0.9977 - binary_accuracy: 0.9776 - val_loss: 0.1736 - val_auc: 0.9850 - val_binary_accuracy: 0.9291\n\nEpoch 00026: val_auc improved from 0.98275 to 0.98503, saving model to Fold_3_1_weights.h5\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0686 - auc: 0.9973 - binary_accuracy: 0.9749 - val_loss: 0.1739 - val_auc: 0.9841 - val_binary_accuracy: 0.9334\n\nEpoch 00027: val_auc did not improve from 0.98503\nEpoch 28/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0787 - auc: 0.9961 - binary_accuracy: 0.9732 - val_loss: 0.1687 - val_auc: 0.9838 - val_binary_accuracy: 0.9365\n\nEpoch 00028: val_auc did not improve from 0.98503\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0835 - auc: 0.9958 - binary_accuracy: 0.9693 - val_loss: 0.1747 - val_auc: 0.9835 - val_binary_accuracy: 0.9336\n\nEpoch 00029: val_auc did not improve from 0.98503\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0864 - auc: 0.9957 - binary_accuracy: 0.9669 - val_loss: 0.1634 - val_auc: 0.9859 - val_binary_accuracy: 0.9338\n\nEpoch 00030: val_auc improved from 0.98503 to 0.98587, saving model to Fold_3_1_weights.h5\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0997 - auc: 0.9941 - binary_accuracy: 0.9620 - val_loss: 0.1591 - val_auc: 0.9862 - val_binary_accuracy: 0.9345\n\nEpoch 00031: val_auc improved from 0.98587 to 0.98617, saving model to Fold_3_1_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1038 - auc: 0.9937 - binary_accuracy: 0.9602 - val_loss: 0.1485 - val_auc: 0.9864 - val_binary_accuracy: 0.9420\n\nEpoch 00032: val_auc improved from 0.98617 to 0.98645, saving model to Fold_3_1_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1153 - auc: 0.9920 - binary_accuracy: 0.9564 - val_loss: 0.1491 - val_auc: 0.9867 - val_binary_accuracy: 0.9426\n\nEpoch 00033: val_auc improved from 0.98645 to 0.98672, saving model to Fold_3_1_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1180 - auc: 0.9917 - binary_accuracy: 0.9525 - val_loss: 0.1607 - val_auc: 0.9854 - val_binary_accuracy: 0.9380\n\nEpoch 00034: val_auc did not improve from 0.98672\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1185 - auc: 0.9917 - binary_accuracy: 0.9532 - val_loss: 0.1690 - val_auc: 0.9860 - val_binary_accuracy: 0.9332\n\nEpoch 00035: val_auc did not improve from 0.98672\nEpoch 36/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1293 - auc: 0.9901 - binary_accuracy: 0.9504 - val_loss: 0.1580 - val_auc: 0.9855 - val_binary_accuracy: 0.9351\n\nEpoch 00036: val_auc did not improve from 0.98672\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1318 - auc: 0.9897 - binary_accuracy: 0.9490 - val_loss: 0.1601 - val_auc: 0.9861 - val_binary_accuracy: 0.9338\n\nEpoch 00037: val_auc did not improve from 0.98672\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1278 - auc: 0.9906 - binary_accuracy: 0.9525 - val_loss: 0.1622 - val_auc: 0.9848 - val_binary_accuracy: 0.9370\n\nEpoch 00038: val_auc did not improve from 0.98672\nEpoch 39/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1362 - auc: 0.9892 - binary_accuracy: 0.9469 - val_loss: 0.1607 - val_auc: 0.9844 - val_binary_accuracy: 0.9370\n\nEpoch 00039: val_auc did not improve from 0.98672\nEpoch 40/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1467 - auc: 0.9873 - binary_accuracy: 0.9445 - val_loss: 0.1650 - val_auc: 0.9842 - val_binary_accuracy: 0.9357\n\nEpoch 00040: val_auc did not improve from 0.98672\n****************************** Fold 3 Trial 2 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 299ms/step - loss: 0.6210 - auc: 0.7493 - binary_accuracy: 0.6850 - val_loss: 0.8725 - val_auc: 0.6157 - val_binary_accuracy: 0.5013\n\nEpoch 00001: val_auc improved from -inf to 0.61568, saving model to Fold_3_2_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.3201 - auc: 0.9370 - binary_accuracy: 0.8589 - val_loss: 0.6922 - val_auc: 0.6550 - val_binary_accuracy: 0.5354\n\nEpoch 00002: val_auc improved from 0.61568 to 0.65504, saving model to Fold_3_2_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.2314 - auc: 0.9675 - binary_accuracy: 0.9050 - val_loss: 0.7926 - val_auc: 0.7817 - val_binary_accuracy: 0.5073\n\nEpoch 00003: val_auc improved from 0.65504 to 0.78174, saving model to Fold_3_2_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2102 - auc: 0.9732 - binary_accuracy: 0.9141 - val_loss: 0.4615 - val_auc: 0.9279 - val_binary_accuracy: 0.7978\n\nEpoch 00004: val_auc improved from 0.78174 to 0.92794, saving model to Fold_3_2_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1932 - auc: 0.9773 - binary_accuracy: 0.9233 - val_loss: 0.4164 - val_auc: 0.9416 - val_binary_accuracy: 0.8025\n\nEpoch 00005: val_auc improved from 0.92794 to 0.94160, saving model to Fold_3_2_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1820 - auc: 0.9798 - binary_accuracy: 0.9286 - val_loss: 0.2974 - val_auc: 0.9590 - val_binary_accuracy: 0.8739\n\nEpoch 00006: val_auc improved from 0.94160 to 0.95897, saving model to Fold_3_2_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1765 - auc: 0.9813 - binary_accuracy: 0.9308 - val_loss: 0.4863 - val_auc: 0.9687 - val_binary_accuracy: 0.7563\n\nEpoch 00007: val_auc improved from 0.95897 to 0.96874, saving model to Fold_3_2_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1495 - auc: 0.9862 - binary_accuracy: 0.9418 - val_loss: 0.2514 - val_auc: 0.9782 - val_binary_accuracy: 0.8908\n\nEpoch 00008: val_auc improved from 0.96874 to 0.97824, saving model to Fold_3_2_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1510 - auc: 0.9861 - binary_accuracy: 0.9398 - val_loss: 0.2614 - val_auc: 0.9758 - val_binary_accuracy: 0.8866\n\nEpoch 00009: val_auc did not improve from 0.97824\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1438 - auc: 0.9875 - binary_accuracy: 0.9421 - val_loss: 0.2479 - val_auc: 0.9787 - val_binary_accuracy: 0.8903\n\nEpoch 00010: val_auc improved from 0.97824 to 0.97872, saving model to Fold_3_2_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1476 - auc: 0.9863 - binary_accuracy: 0.9413 - val_loss: 0.2777 - val_auc: 0.9791 - val_binary_accuracy: 0.8781\n\nEpoch 00011: val_auc improved from 0.97872 to 0.97913, saving model to Fold_3_2_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1446 - auc: 0.9872 - binary_accuracy: 0.9421 - val_loss: 0.2208 - val_auc: 0.9835 - val_binary_accuracy: 0.9024\n\nEpoch 00012: val_auc improved from 0.97913 to 0.98354, saving model to Fold_3_2_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1376 - auc: 0.9883 - binary_accuracy: 0.9468 - val_loss: 0.1692 - val_auc: 0.9839 - val_binary_accuracy: 0.9342\n\nEpoch 00013: val_auc improved from 0.98354 to 0.98389, saving model to Fold_3_2_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1273 - auc: 0.9897 - binary_accuracy: 0.9502 - val_loss: 0.1823 - val_auc: 0.9798 - val_binary_accuracy: 0.9270\n\nEpoch 00014: val_auc did not improve from 0.98389\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1278 - auc: 0.9899 - binary_accuracy: 0.9486 - val_loss: 0.1721 - val_auc: 0.9835 - val_binary_accuracy: 0.9347\n\nEpoch 00015: val_auc did not improve from 0.98389\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1260 - auc: 0.9903 - binary_accuracy: 0.9500 - val_loss: 0.1922 - val_auc: 0.9841 - val_binary_accuracy: 0.9203\n\nEpoch 00016: val_auc improved from 0.98389 to 0.98412, saving model to Fold_3_2_weights.h5\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1279 - auc: 0.9900 - binary_accuracy: 0.9486 - val_loss: 0.2153 - val_auc: 0.9810 - val_binary_accuracy: 0.9126\n\nEpoch 00017: val_auc did not improve from 0.98412\nEpoch 18/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1222 - auc: 0.9904 - binary_accuracy: 0.9518 - val_loss: 0.2301 - val_auc: 0.9753 - val_binary_accuracy: 0.9126\n\nEpoch 00018: val_auc did not improve from 0.98412\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1201 - auc: 0.9910 - binary_accuracy: 0.9527 - val_loss: 0.1577 - val_auc: 0.9849 - val_binary_accuracy: 0.9420\n\nEpoch 00019: val_auc improved from 0.98412 to 0.98492, saving model to Fold_3_2_weights.h5\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1333 - auc: 0.9892 - binary_accuracy: 0.9468 - val_loss: 0.2257 - val_auc: 0.9801 - val_binary_accuracy: 0.9112\n\nEpoch 00020: val_auc did not improve from 0.98492\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1218 - auc: 0.9907 - binary_accuracy: 0.9499 - val_loss: 0.1692 - val_auc: 0.9839 - val_binary_accuracy: 0.9345\n\nEpoch 00021: val_auc did not improve from 0.98492\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1182 - auc: 0.9912 - binary_accuracy: 0.9540 - val_loss: 0.2122 - val_auc: 0.9807 - val_binary_accuracy: 0.9197\n\nEpoch 00022: val_auc did not improve from 0.98492\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1107 - auc: 0.9925 - binary_accuracy: 0.9570 - val_loss: 0.2023 - val_auc: 0.9829 - val_binary_accuracy: 0.9230\n\nEpoch 00023: val_auc did not improve from 0.98492\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0967 - auc: 0.9941 - binary_accuracy: 0.9631 - val_loss: 0.1768 - val_auc: 0.9848 - val_binary_accuracy: 0.9338\n\nEpoch 00024: val_auc did not improve from 0.98492\nEpoch 25/40\n82/82 [==============================] - 6s 78ms/step - loss: 0.0825 - auc: 0.9958 - binary_accuracy: 0.9669 - val_loss: 0.2219 - val_auc: 0.9804 - val_binary_accuracy: 0.9209\n\nEpoch 00025: val_auc did not improve from 0.98492\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0880 - auc: 0.9949 - binary_accuracy: 0.9660 - val_loss: 0.2096 - val_auc: 0.9785 - val_binary_accuracy: 0.9214\n\nEpoch 00026: val_auc did not improve from 0.98492\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0822 - auc: 0.9955 - binary_accuracy: 0.9677 - val_loss: 0.1671 - val_auc: 0.9832 - val_binary_accuracy: 0.9347\n\nEpoch 00027: val_auc did not improve from 0.98492\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 28/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0770 - auc: 0.9962 - binary_accuracy: 0.9718 - val_loss: 0.1688 - val_auc: 0.9859 - val_binary_accuracy: 0.9338\n\nEpoch 00028: val_auc improved from 0.98492 to 0.98589, saving model to Fold_3_2_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0657 - auc: 0.9974 - binary_accuracy: 0.9751 - val_loss: 0.2541 - val_auc: 0.9801 - val_binary_accuracy: 0.9012\n\nEpoch 00029: val_auc did not improve from 0.98589\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0740 - auc: 0.9967 - binary_accuracy: 0.9722 - val_loss: 0.2221 - val_auc: 0.9820 - val_binary_accuracy: 0.9153\n\nEpoch 00030: val_auc did not improve from 0.98589\nEpoch 31/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0838 - auc: 0.9958 - binary_accuracy: 0.9702 - val_loss: 0.1563 - val_auc: 0.9854 - val_binary_accuracy: 0.9405\n\nEpoch 00031: val_auc did not improve from 0.98589\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0751 - auc: 0.9965 - binary_accuracy: 0.9720 - val_loss: 0.1544 - val_auc: 0.9858 - val_binary_accuracy: 0.9415\n\nEpoch 00032: val_auc did not improve from 0.98589\n\nEpoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0763 - auc: 0.9967 - binary_accuracy: 0.9722 - val_loss: 0.1458 - val_auc: 0.9874 - val_binary_accuracy: 0.9469\n\nEpoch 00033: val_auc improved from 0.98589 to 0.98741, saving model to Fold_3_2_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0803 - auc: 0.9963 - binary_accuracy: 0.9694 - val_loss: 0.1580 - val_auc: 0.9864 - val_binary_accuracy: 0.9392\n\nEpoch 00034: val_auc did not improve from 0.98741\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0864 - auc: 0.9955 - binary_accuracy: 0.9685 - val_loss: 0.1677 - val_auc: 0.9848 - val_binary_accuracy: 0.9382\n\nEpoch 00035: val_auc did not improve from 0.98741\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0982 - auc: 0.9941 - binary_accuracy: 0.9628 - val_loss: 0.1551 - val_auc: 0.9860 - val_binary_accuracy: 0.9407\n\nEpoch 00036: val_auc did not improve from 0.98741\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1041 - auc: 0.9936 - binary_accuracy: 0.9593 - val_loss: 0.1470 - val_auc: 0.9864 - val_binary_accuracy: 0.9413\n\nEpoch 00037: val_auc did not improve from 0.98741\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1093 - auc: 0.9930 - binary_accuracy: 0.9587 - val_loss: 0.1507 - val_auc: 0.9863 - val_binary_accuracy: 0.9424\n\nEpoch 00038: val_auc did not improve from 0.98741\nEpoch 39/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1199 - auc: 0.9918 - binary_accuracy: 0.9549 - val_loss: 0.1584 - val_auc: 0.9851 - val_binary_accuracy: 0.9390\n\nEpoch 00039: val_auc did not improve from 0.98741\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1328 - auc: 0.9901 - binary_accuracy: 0.9465 - val_loss: 0.1696 - val_auc: 0.9840 - val_binary_accuracy: 0.9351\n\nEpoch 00040: val_auc did not improve from 0.98741\n****************************** Fold 3 Trial 3 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 297ms/step - loss: 0.6985 - auc: 0.7157 - binary_accuracy: 0.6623 - val_loss: 0.9408 - val_auc: 0.7173 - val_binary_accuracy: 0.5391\n\nEpoch 00001: val_auc improved from -inf to 0.71727, saving model to Fold_3_3_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2879 - auc: 0.9490 - binary_accuracy: 0.8777 - val_loss: 0.9432 - val_auc: 0.7858 - val_binary_accuracy: 0.5672\n\nEpoch 00002: val_auc improved from 0.71727 to 0.78580, saving model to Fold_3_3_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2375 - auc: 0.9658 - binary_accuracy: 0.8997 - val_loss: 0.8775 - val_auc: 0.8274 - val_binary_accuracy: 0.5965\n\nEpoch 00003: val_auc improved from 0.78580 to 0.82744, saving model to Fold_3_3_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1989 - auc: 0.9760 - binary_accuracy: 0.9174 - val_loss: 0.9542 - val_auc: 0.8167 - val_binary_accuracy: 0.5909\n\nEpoch 00004: val_auc did not improve from 0.82744\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1867 - auc: 0.9788 - binary_accuracy: 0.9261 - val_loss: 0.6481 - val_auc: 0.8866 - val_binary_accuracy: 0.6975\n\nEpoch 00005: val_auc improved from 0.82744 to 0.88660, saving model to Fold_3_3_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1777 - auc: 0.9809 - binary_accuracy: 0.9283 - val_loss: 0.8254 - val_auc: 0.9166 - val_binary_accuracy: 0.6962\n\nEpoch 00006: val_auc improved from 0.88660 to 0.91663, saving model to Fold_3_3_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1557 - auc: 0.9854 - binary_accuracy: 0.9365 - val_loss: 0.3376 - val_auc: 0.9590 - val_binary_accuracy: 0.8527\n\nEpoch 00007: val_auc improved from 0.91663 to 0.95898, saving model to Fold_3_3_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1536 - auc: 0.9854 - binary_accuracy: 0.9400 - val_loss: 0.2649 - val_auc: 0.9694 - val_binary_accuracy: 0.8879\n\nEpoch 00008: val_auc improved from 0.95898 to 0.96938, saving model to Fold_3_3_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1409 - auc: 0.9877 - binary_accuracy: 0.9443 - val_loss: 0.2059 - val_auc: 0.9744 - val_binary_accuracy: 0.9189\n\nEpoch 00009: val_auc improved from 0.96938 to 0.97440, saving model to Fold_3_3_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1404 - auc: 0.9879 - binary_accuracy: 0.9447 - val_loss: 0.2035 - val_auc: 0.9783 - val_binary_accuracy: 0.9203\n\nEpoch 00010: val_auc improved from 0.97440 to 0.97828, saving model to Fold_3_3_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1322 - auc: 0.9894 - binary_accuracy: 0.9446 - val_loss: 0.2078 - val_auc: 0.9765 - val_binary_accuracy: 0.9155\n\nEpoch 00011: val_auc did not improve from 0.97828\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1251 - auc: 0.9906 - binary_accuracy: 0.9507 - val_loss: 0.2292 - val_auc: 0.9748 - val_binary_accuracy: 0.9074\n\nEpoch 00012: val_auc did not improve from 0.97828\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1295 - auc: 0.9894 - binary_accuracy: 0.9485 - val_loss: 0.1893 - val_auc: 0.9805 - val_binary_accuracy: 0.9253\n\nEpoch 00013: val_auc improved from 0.97828 to 0.98054, saving model to Fold_3_3_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1429 - auc: 0.9877 - binary_accuracy: 0.9413 - val_loss: 0.2243 - val_auc: 0.9779 - val_binary_accuracy: 0.9103\n\nEpoch 00014: val_auc did not improve from 0.98054\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1457 - auc: 0.9870 - binary_accuracy: 0.9428 - val_loss: 0.1833 - val_auc: 0.9799 - val_binary_accuracy: 0.9288\n\nEpoch 00015: val_auc did not improve from 0.98054\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1137 - auc: 0.9918 - binary_accuracy: 0.9560 - val_loss: 0.1955 - val_auc: 0.9782 - val_binary_accuracy: 0.9257\n\nEpoch 00016: val_auc did not improve from 0.98054\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1113 - auc: 0.9924 - binary_accuracy: 0.9562 - val_loss: 0.1886 - val_auc: 0.9809 - val_binary_accuracy: 0.9303\n\nEpoch 00017: val_auc improved from 0.98054 to 0.98087, saving model to Fold_3_3_weights.h5\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1082 - auc: 0.9922 - binary_accuracy: 0.9595 - val_loss: 0.1564 - val_auc: 0.9851 - val_binary_accuracy: 0.9388\n\nEpoch 00018: val_auc improved from 0.98087 to 0.98512, saving model to Fold_3_3_weights.h5\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1183 - auc: 0.9912 - binary_accuracy: 0.9521 - val_loss: 0.1925 - val_auc: 0.9817 - val_binary_accuracy: 0.9286\n\nEpoch 00019: val_auc did not improve from 0.98512\nEpoch 20/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1139 - auc: 0.9922 - binary_accuracy: 0.9552 - val_loss: 0.1765 - val_auc: 0.9832 - val_binary_accuracy: 0.9318\n\nEpoch 00020: val_auc did not improve from 0.98512\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0930 - auc: 0.9944 - binary_accuracy: 0.9649 - val_loss: 0.2222 - val_auc: 0.9737 - val_binary_accuracy: 0.9132\n\nEpoch 00021: val_auc did not improve from 0.98512\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1018 - auc: 0.9935 - binary_accuracy: 0.9618 - val_loss: 0.1822 - val_auc: 0.9806 - val_binary_accuracy: 0.9324\n\nEpoch 00022: val_auc did not improve from 0.98512\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0830 - auc: 0.9957 - binary_accuracy: 0.9682 - val_loss: 0.1952 - val_auc: 0.9816 - val_binary_accuracy: 0.9305\n\nEpoch 00023: val_auc did not improve from 0.98512\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0560 - auc: 0.9980 - binary_accuracy: 0.9808 - val_loss: 0.2018 - val_auc: 0.9787 - val_binary_accuracy: 0.9290\n\nEpoch 00024: val_auc did not improve from 0.98512\nEpoch 25/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0652 - auc: 0.9970 - binary_accuracy: 0.9766 - val_loss: 0.2152 - val_auc: 0.9800 - val_binary_accuracy: 0.9209\n\nEpoch 00025: val_auc did not improve from 0.98512\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0590 - auc: 0.9977 - binary_accuracy: 0.9780 - val_loss: 0.1852 - val_auc: 0.9824 - val_binary_accuracy: 0.9328\n\nEpoch 00026: val_auc did not improve from 0.98512\n\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 27/40\n82/82 [==============================] - 6s 76ms/step - loss: 0.0538 - auc: 0.9982 - binary_accuracy: 0.9802 - val_loss: 0.1959 - val_auc: 0.9809 - val_binary_accuracy: 0.9276\n\nEpoch 00027: val_auc did not improve from 0.98512\nEpoch 28/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0516 - auc: 0.9985 - binary_accuracy: 0.9812 - val_loss: 0.1794 - val_auc: 0.9826 - val_binary_accuracy: 0.9347\n\nEpoch 00028: val_auc did not improve from 0.98512\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0530 - auc: 0.9983 - binary_accuracy: 0.9812 - val_loss: 0.1700 - val_auc: 0.9839 - val_binary_accuracy: 0.9378\n\nEpoch 00029: val_auc did not improve from 0.98512\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0610 - auc: 0.9979 - binary_accuracy: 0.9775 - val_loss: 0.1852 - val_auc: 0.9811 - val_binary_accuracy: 0.9324\n\nEpoch 00030: val_auc did not improve from 0.98512\n\nEpoch 00030: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 31/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0521 - auc: 0.9986 - binary_accuracy: 0.9828 - val_loss: 0.1607 - val_auc: 0.9850 - val_binary_accuracy: 0.9386\n\nEpoch 00031: val_auc did not improve from 0.98512\nEpoch 32/40\n82/82 [==============================] - 6s 78ms/step - loss: 0.0588 - auc: 0.9982 - binary_accuracy: 0.9798 - val_loss: 0.1570 - val_auc: 0.9854 - val_binary_accuracy: 0.9413\n\nEpoch 00032: val_auc improved from 0.98512 to 0.98537, saving model to Fold_3_3_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0626 - auc: 0.9980 - binary_accuracy: 0.9773 - val_loss: 0.1632 - val_auc: 0.9846 - val_binary_accuracy: 0.9345\n\nEpoch 00033: val_auc did not improve from 0.98537\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0790 - auc: 0.9964 - binary_accuracy: 0.9714 - val_loss: 0.1652 - val_auc: 0.9839 - val_binary_accuracy: 0.9367\n\nEpoch 00034: val_auc did not improve from 0.98537\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0820 - auc: 0.9961 - binary_accuracy: 0.9694 - val_loss: 0.1636 - val_auc: 0.9852 - val_binary_accuracy: 0.9353\n\nEpoch 00035: val_auc did not improve from 0.98537\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0954 - auc: 0.9947 - binary_accuracy: 0.9649 - val_loss: 0.1579 - val_auc: 0.9852 - val_binary_accuracy: 0.9365\n\nEpoch 00036: val_auc did not improve from 0.98537\n\nEpoch 00036: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0964 - auc: 0.9947 - binary_accuracy: 0.9624 - val_loss: 0.1535 - val_auc: 0.9856 - val_binary_accuracy: 0.9388\n\nEpoch 00037: val_auc improved from 0.98537 to 0.98562, saving model to Fold_3_3_weights.h5\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1054 - auc: 0.9937 - binary_accuracy: 0.9601 - val_loss: 0.1614 - val_auc: 0.9844 - val_binary_accuracy: 0.9355\n\nEpoch 00038: val_auc did not improve from 0.98562\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1218 - auc: 0.9918 - binary_accuracy: 0.9532 - val_loss: 0.1602 - val_auc: 0.9847 - val_binary_accuracy: 0.9380\n\nEpoch 00039: val_auc did not improve from 0.98562\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1314 - auc: 0.9900 - binary_accuracy: 0.9498 - val_loss: 0.1701 - val_auc: 0.9825 - val_binary_accuracy: 0.9315\n\nEpoch 00040: val_auc did not improve from 0.98562\n****************************** Fold 4 Trial 1 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 300ms/step - loss: 0.6339 - auc: 0.7413 - binary_accuracy: 0.6805 - val_loss: 1.0800 - val_auc: 0.7402 - val_binary_accuracy: 0.5178\n\nEpoch 00001: val_auc improved from -inf to 0.74016, saving model to Fold_4_1_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.3107 - auc: 0.9407 - binary_accuracy: 0.8694 - val_loss: 1.0935 - val_auc: 0.8708 - val_binary_accuracy: 0.5432\n\nEpoch 00002: val_auc improved from 0.74016 to 0.87075, saving model to Fold_4_1_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.2361 - auc: 0.9660 - binary_accuracy: 0.9018 - val_loss: 0.7315 - val_auc: 0.8779 - val_binary_accuracy: 0.6247\n\nEpoch 00003: val_auc improved from 0.87075 to 0.87795, saving model to Fold_4_1_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1952 - auc: 0.9769 - binary_accuracy: 0.9207 - val_loss: 0.8381 - val_auc: 0.8746 - val_binary_accuracy: 0.6318\n\nEpoch 00004: val_auc did not improve from 0.87795\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1705 - auc: 0.9823 - binary_accuracy: 0.9315 - val_loss: 0.5916 - val_auc: 0.9154 - val_binary_accuracy: 0.7115\n\nEpoch 00005: val_auc improved from 0.87795 to 0.91540, saving model to Fold_4_1_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1688 - auc: 0.9829 - binary_accuracy: 0.9325 - val_loss: 0.5680 - val_auc: 0.9394 - val_binary_accuracy: 0.7772\n\nEpoch 00006: val_auc improved from 0.91540 to 0.93936, saving model to Fold_4_1_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1490 - auc: 0.9864 - binary_accuracy: 0.9377 - val_loss: 0.2442 - val_auc: 0.9658 - val_binary_accuracy: 0.8999\n\nEpoch 00007: val_auc improved from 0.93936 to 0.96583, saving model to Fold_4_1_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1580 - auc: 0.9851 - binary_accuracy: 0.9351 - val_loss: 0.3278 - val_auc: 0.9627 - val_binary_accuracy: 0.8644\n\nEpoch 00008: val_auc did not improve from 0.96583\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1476 - auc: 0.9866 - binary_accuracy: 0.9421 - val_loss: 0.1912 - val_auc: 0.9838 - val_binary_accuracy: 0.9201\n\nEpoch 00009: val_auc improved from 0.96583 to 0.98384, saving model to Fold_4_1_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.1377 - auc: 0.9883 - binary_accuracy: 0.9453 - val_loss: 0.2189 - val_auc: 0.9805 - val_binary_accuracy: 0.9091\n\nEpoch 00010: val_auc did not improve from 0.98384\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1334 - auc: 0.9892 - binary_accuracy: 0.9458 - val_loss: 0.2194 - val_auc: 0.9745 - val_binary_accuracy: 0.9139\n\nEpoch 00011: val_auc did not improve from 0.98384\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1153 - auc: 0.9919 - binary_accuracy: 0.9564 - val_loss: 0.1881 - val_auc: 0.9801 - val_binary_accuracy: 0.9228\n\nEpoch 00012: val_auc did not improve from 0.98384\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1356 - auc: 0.9884 - binary_accuracy: 0.9470 - val_loss: 0.2745 - val_auc: 0.9770 - val_binary_accuracy: 0.8920\n\nEpoch 00013: val_auc did not improve from 0.98384\n\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 14/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1238 - auc: 0.9907 - binary_accuracy: 0.9501 - val_loss: 0.1781 - val_auc: 0.9821 - val_binary_accuracy: 0.9307\n\nEpoch 00014: val_auc did not improve from 0.98384\nEpoch 15/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0939 - auc: 0.9945 - binary_accuracy: 0.9639 - val_loss: 0.1745 - val_auc: 0.9845 - val_binary_accuracy: 0.9286\n\nEpoch 00015: val_auc improved from 0.98384 to 0.98453, saving model to Fold_4_1_weights.h5\nEpoch 16/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.0937 - auc: 0.9946 - binary_accuracy: 0.9645 - val_loss: 0.1838 - val_auc: 0.9852 - val_binary_accuracy: 0.9239\n\nEpoch 00016: val_auc improved from 0.98453 to 0.98518, saving model to Fold_4_1_weights.h5\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1037 - auc: 0.9931 - binary_accuracy: 0.9589 - val_loss: 0.2040 - val_auc: 0.9784 - val_binary_accuracy: 0.9180\n\nEpoch 00017: val_auc did not improve from 0.98518\nEpoch 18/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0930 - auc: 0.9948 - binary_accuracy: 0.9654 - val_loss: 0.2254 - val_auc: 0.9801 - val_binary_accuracy: 0.9141\n\nEpoch 00018: val_auc did not improve from 0.98518\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0992 - auc: 0.9934 - binary_accuracy: 0.9598 - val_loss: 0.2046 - val_auc: 0.9805 - val_binary_accuracy: 0.9178\n\nEpoch 00019: val_auc did not improve from 0.98518\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0960 - auc: 0.9942 - binary_accuracy: 0.9639 - val_loss: 0.2034 - val_auc: 0.9827 - val_binary_accuracy: 0.9185\n\nEpoch 00020: val_auc did not improve from 0.98518\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0802 - auc: 0.9961 - binary_accuracy: 0.9709 - val_loss: 0.1921 - val_auc: 0.9831 - val_binary_accuracy: 0.9264\n\nEpoch 00021: val_auc did not improve from 0.98518\nEpoch 22/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0787 - auc: 0.9964 - binary_accuracy: 0.9690 - val_loss: 0.1776 - val_auc: 0.9824 - val_binary_accuracy: 0.9293\n\nEpoch 00022: val_auc did not improve from 0.98518\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0799 - auc: 0.9963 - binary_accuracy: 0.9698 - val_loss: 0.1907 - val_auc: 0.9828 - val_binary_accuracy: 0.9299\n\nEpoch 00023: val_auc did not improve from 0.98518\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0845 - auc: 0.9956 - binary_accuracy: 0.9665 - val_loss: 0.1697 - val_auc: 0.9846 - val_binary_accuracy: 0.9326\n\nEpoch 00024: val_auc did not improve from 0.98518\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0755 - auc: 0.9966 - binary_accuracy: 0.9729 - val_loss: 0.1727 - val_auc: 0.9857 - val_binary_accuracy: 0.9322\n\nEpoch 00025: val_auc improved from 0.98518 to 0.98565, saving model to Fold_4_1_weights.h5\nEpoch 26/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0755 - auc: 0.9966 - binary_accuracy: 0.9712 - val_loss: 0.1554 - val_auc: 0.9855 - val_binary_accuracy: 0.9372\n\nEpoch 00026: val_auc did not improve from 0.98565\nEpoch 27/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0850 - auc: 0.9959 - binary_accuracy: 0.9673 - val_loss: 0.1538 - val_auc: 0.9863 - val_binary_accuracy: 0.9372\n\nEpoch 00027: val_auc improved from 0.98565 to 0.98629, saving model to Fold_4_1_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1014 - auc: 0.9941 - binary_accuracy: 0.9606 - val_loss: 0.1641 - val_auc: 0.9855 - val_binary_accuracy: 0.9332\n\nEpoch 00028: val_auc did not improve from 0.98629\nEpoch 29/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0970 - auc: 0.9945 - binary_accuracy: 0.9619 - val_loss: 0.1473 - val_auc: 0.9868 - val_binary_accuracy: 0.9413\n\nEpoch 00029: val_auc improved from 0.98629 to 0.98679, saving model to Fold_4_1_weights.h5\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1048 - auc: 0.9936 - binary_accuracy: 0.9594 - val_loss: 0.1691 - val_auc: 0.9858 - val_binary_accuracy: 0.9314\n\nEpoch 00030: val_auc did not improve from 0.98679\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1077 - auc: 0.9930 - binary_accuracy: 0.9569 - val_loss: 0.1701 - val_auc: 0.9863 - val_binary_accuracy: 0.9303\n\nEpoch 00031: val_auc did not improve from 0.98679\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1229 - auc: 0.9913 - binary_accuracy: 0.9497 - val_loss: 0.1426 - val_auc: 0.9874 - val_binary_accuracy: 0.9432\n\nEpoch 00032: val_auc improved from 0.98679 to 0.98742, saving model to Fold_4_1_weights.h5\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1161 - auc: 0.9919 - binary_accuracy: 0.9559 - val_loss: 0.1476 - val_auc: 0.9867 - val_binary_accuracy: 0.9417\n\nEpoch 00033: val_auc did not improve from 0.98742\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1238 - auc: 0.9910 - binary_accuracy: 0.9519 - val_loss: 0.1668 - val_auc: 0.9860 - val_binary_accuracy: 0.9303\n\nEpoch 00034: val_auc did not improve from 0.98742\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1356 - auc: 0.9893 - binary_accuracy: 0.9476 - val_loss: 0.1731 - val_auc: 0.9864 - val_binary_accuracy: 0.9299\n\nEpoch 00035: val_auc did not improve from 0.98742\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1334 - auc: 0.9893 - binary_accuracy: 0.9493 - val_loss: 0.1690 - val_auc: 0.9851 - val_binary_accuracy: 0.9318\n\nEpoch 00036: val_auc did not improve from 0.98742\n\nEpoch 00036: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1237 - auc: 0.9908 - binary_accuracy: 0.9526 - val_loss: 0.1590 - val_auc: 0.9858 - val_binary_accuracy: 0.9368\n\nEpoch 00037: val_auc did not improve from 0.98742\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1400 - auc: 0.9888 - binary_accuracy: 0.9460 - val_loss: 0.1575 - val_auc: 0.9852 - val_binary_accuracy: 0.9363\n\nEpoch 00038: val_auc did not improve from 0.98742\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1544 - auc: 0.9858 - binary_accuracy: 0.9385 - val_loss: 0.1718 - val_auc: 0.9827 - val_binary_accuracy: 0.9301\n\nEpoch 00039: val_auc did not improve from 0.98742\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1554 - auc: 0.9859 - binary_accuracy: 0.9382 - val_loss: 0.1704 - val_auc: 0.9829 - val_binary_accuracy: 0.9326\n\nEpoch 00040: val_auc did not improve from 0.98742\n\nEpoch 00040: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n****************************** Fold 4 Trial 2 ******************************\nEpoch 1/40\n82/82 [==============================] - 53s 298ms/step - loss: 0.6289 - auc: 0.7244 - binary_accuracy: 0.6654 - val_loss: 1.4377 - val_auc: 0.7920 - val_binary_accuracy: 0.5047\n\nEpoch 00001: val_auc improved from -inf to 0.79199, saving model to Fold_4_2_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2960 - auc: 0.9462 - binary_accuracy: 0.8785 - val_loss: 1.2377 - val_auc: 0.8648 - val_binary_accuracy: 0.5423\n\nEpoch 00002: val_auc improved from 0.79199 to 0.86482, saving model to Fold_4_2_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2108 - auc: 0.9732 - binary_accuracy: 0.9121 - val_loss: 1.2600 - val_auc: 0.8615 - val_binary_accuracy: 0.5906\n\nEpoch 00003: val_auc did not improve from 0.86482\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1893 - auc: 0.9782 - binary_accuracy: 0.9227 - val_loss: 0.9199 - val_auc: 0.8978 - val_binary_accuracy: 0.6359\n\nEpoch 00004: val_auc improved from 0.86482 to 0.89780, saving model to Fold_4_2_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1666 - auc: 0.9831 - binary_accuracy: 0.9328 - val_loss: 0.3782 - val_auc: 0.9663 - val_binary_accuracy: 0.8359\n\nEpoch 00005: val_auc improved from 0.89780 to 0.96631, saving model to Fold_4_2_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1569 - auc: 0.9850 - binary_accuracy: 0.9387 - val_loss: 0.3492 - val_auc: 0.9599 - val_binary_accuracy: 0.8392\n\nEpoch 00006: val_auc did not improve from 0.96631\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1677 - auc: 0.9829 - binary_accuracy: 0.9309 - val_loss: 0.4963 - val_auc: 0.9547 - val_binary_accuracy: 0.7955\n\nEpoch 00007: val_auc did not improve from 0.96631\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1790 - auc: 0.9804 - binary_accuracy: 0.9293 - val_loss: 0.2528 - val_auc: 0.9732 - val_binary_accuracy: 0.8949\n\nEpoch 00008: val_auc improved from 0.96631 to 0.97324, saving model to Fold_4_2_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1389 - auc: 0.9881 - binary_accuracy: 0.9459 - val_loss: 0.2571 - val_auc: 0.9766 - val_binary_accuracy: 0.8976\n\nEpoch 00009: val_auc improved from 0.97324 to 0.97661, saving model to Fold_4_2_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1486 - auc: 0.9863 - binary_accuracy: 0.9410 - val_loss: 0.2125 - val_auc: 0.9759 - val_binary_accuracy: 0.9216\n\nEpoch 00010: val_auc did not improve from 0.97661\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1329 - auc: 0.9889 - binary_accuracy: 0.9469 - val_loss: 0.2281 - val_auc: 0.9757 - val_binary_accuracy: 0.9093\n\nEpoch 00011: val_auc did not improve from 0.97661\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1292 - auc: 0.9897 - binary_accuracy: 0.9499 - val_loss: 0.1913 - val_auc: 0.9797 - val_binary_accuracy: 0.9222\n\nEpoch 00012: val_auc improved from 0.97661 to 0.97966, saving model to Fold_4_2_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1136 - auc: 0.9919 - binary_accuracy: 0.9563 - val_loss: 0.2238 - val_auc: 0.9769 - val_binary_accuracy: 0.9068\n\nEpoch 00013: val_auc did not improve from 0.97966\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1117 - auc: 0.9924 - binary_accuracy: 0.9567 - val_loss: 0.2184 - val_auc: 0.9840 - val_binary_accuracy: 0.9095\n\nEpoch 00014: val_auc improved from 0.97966 to 0.98400, saving model to Fold_4_2_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1147 - auc: 0.9918 - binary_accuracy: 0.9540 - val_loss: 0.2325 - val_auc: 0.9793 - val_binary_accuracy: 0.9133\n\nEpoch 00015: val_auc did not improve from 0.98400\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1146 - auc: 0.9921 - binary_accuracy: 0.9527 - val_loss: 0.2655 - val_auc: 0.9799 - val_binary_accuracy: 0.8993\n\nEpoch 00016: val_auc did not improve from 0.98400\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1067 - auc: 0.9931 - binary_accuracy: 0.9587 - val_loss: 0.2025 - val_auc: 0.9803 - val_binary_accuracy: 0.9197\n\nEpoch 00017: val_auc did not improve from 0.98400\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1001 - auc: 0.9937 - binary_accuracy: 0.9607 - val_loss: 0.2236 - val_auc: 0.9743 - val_binary_accuracy: 0.9176\n\nEpoch 00018: val_auc did not improve from 0.98400\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 19/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0903 - auc: 0.9949 - binary_accuracy: 0.9665 - val_loss: 0.2571 - val_auc: 0.9832 - val_binary_accuracy: 0.8987\n\nEpoch 00019: val_auc did not improve from 0.98400\nEpoch 20/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0721 - auc: 0.9968 - binary_accuracy: 0.9729 - val_loss: 0.1962 - val_auc: 0.9810 - val_binary_accuracy: 0.9234\n\nEpoch 00020: val_auc did not improve from 0.98400\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0626 - auc: 0.9974 - binary_accuracy: 0.9772 - val_loss: 0.2216 - val_auc: 0.9797 - val_binary_accuracy: 0.9210\n\nEpoch 00021: val_auc did not improve from 0.98400\nEpoch 22/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0753 - auc: 0.9964 - binary_accuracy: 0.9707 - val_loss: 0.2087 - val_auc: 0.9826 - val_binary_accuracy: 0.9184\n\nEpoch 00022: val_auc did not improve from 0.98400\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0657 - auc: 0.9974 - binary_accuracy: 0.9760 - val_loss: 0.1863 - val_auc: 0.9827 - val_binary_accuracy: 0.9289\n\nEpoch 00023: val_auc did not improve from 0.98400\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0541 - auc: 0.9982 - binary_accuracy: 0.9812 - val_loss: 0.2144 - val_auc: 0.9797 - val_binary_accuracy: 0.9168\n\nEpoch 00024: val_auc did not improve from 0.98400\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0573 - auc: 0.9982 - binary_accuracy: 0.9783 - val_loss: 0.2084 - val_auc: 0.9820 - val_binary_accuracy: 0.9187\n\nEpoch 00025: val_auc did not improve from 0.98400\nEpoch 26/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0620 - auc: 0.9976 - binary_accuracy: 0.9779 - val_loss: 0.2125 - val_auc: 0.9812 - val_binary_accuracy: 0.9199\n\nEpoch 00026: val_auc did not improve from 0.98400\n\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 27/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0575 - auc: 0.9983 - binary_accuracy: 0.9796 - val_loss: 0.1800 - val_auc: 0.9842 - val_binary_accuracy: 0.9276\n\nEpoch 00027: val_auc improved from 0.98400 to 0.98419, saving model to Fold_4_2_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0644 - auc: 0.9978 - binary_accuracy: 0.9774 - val_loss: 0.1514 - val_auc: 0.9871 - val_binary_accuracy: 0.9388\n\nEpoch 00028: val_auc improved from 0.98419 to 0.98706, saving model to Fold_4_2_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0683 - auc: 0.9974 - binary_accuracy: 0.9752 - val_loss: 0.1543 - val_auc: 0.9859 - val_binary_accuracy: 0.9388\n\nEpoch 00029: val_auc did not improve from 0.98706\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0799 - auc: 0.9963 - binary_accuracy: 0.9713 - val_loss: 0.1491 - val_auc: 0.9871 - val_binary_accuracy: 0.9424\n\nEpoch 00030: val_auc improved from 0.98706 to 0.98713, saving model to Fold_4_2_weights.h5\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0863 - auc: 0.9958 - binary_accuracy: 0.9670 - val_loss: 0.1429 - val_auc: 0.9880 - val_binary_accuracy: 0.9432\n\nEpoch 00031: val_auc improved from 0.98713 to 0.98801, saving model to Fold_4_2_weights.h5\nEpoch 32/40\n82/82 [==============================] - 6s 72ms/step - loss: 0.0940 - auc: 0.9948 - binary_accuracy: 0.9649 - val_loss: 0.1462 - val_auc: 0.9873 - val_binary_accuracy: 0.9447\n\nEpoch 00032: val_auc did not improve from 0.98801\nEpoch 33/40\n82/82 [==============================] - 6s 73ms/step - loss: 0.0957 - auc: 0.9947 - binary_accuracy: 0.9637 - val_loss: 0.1440 - val_auc: 0.9876 - val_binary_accuracy: 0.9449\n\nEpoch 00033: val_auc did not improve from 0.98801\nEpoch 34/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1127 - auc: 0.9926 - binary_accuracy: 0.9553 - val_loss: 0.1455 - val_auc: 0.9873 - val_binary_accuracy: 0.9388\n\nEpoch 00034: val_auc did not improve from 0.98801\nEpoch 35/40\n82/82 [==============================] - 6s 79ms/step - loss: 0.1096 - auc: 0.9928 - binary_accuracy: 0.9583 - val_loss: 0.1700 - val_auc: 0.9854 - val_binary_accuracy: 0.9309\n\nEpoch 00035: val_auc did not improve from 0.98801\n\nEpoch 00035: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1175 - auc: 0.9922 - binary_accuracy: 0.9554 - val_loss: 0.1493 - val_auc: 0.9866 - val_binary_accuracy: 0.9426\n\nEpoch 00036: val_auc did not improve from 0.98801\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1287 - auc: 0.9905 - binary_accuracy: 0.9508 - val_loss: 0.1582 - val_auc: 0.9859 - val_binary_accuracy: 0.9380\n\nEpoch 00037: val_auc did not improve from 0.98801\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1342 - auc: 0.9897 - binary_accuracy: 0.9480 - val_loss: 0.1584 - val_auc: 0.9850 - val_binary_accuracy: 0.9363\n\nEpoch 00038: val_auc did not improve from 0.98801\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1439 - auc: 0.9880 - binary_accuracy: 0.9450 - val_loss: 0.1726 - val_auc: 0.9824 - val_binary_accuracy: 0.9305\n\nEpoch 00039: val_auc did not improve from 0.98801\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 40/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1588 - auc: 0.9853 - binary_accuracy: 0.9400 - val_loss: 0.1808 - val_auc: 0.9808 - val_binary_accuracy: 0.9299\n\nEpoch 00040: val_auc did not improve from 0.98801\n****************************** Fold 4 Trial 3 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 303ms/step - loss: 0.7242 - auc: 0.7075 - binary_accuracy: 0.6537 - val_loss: 0.6640 - val_auc: 0.7653 - val_binary_accuracy: 0.6307\n\nEpoch 00001: val_auc improved from -inf to 0.76535, saving model to Fold_4_3_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3687 - auc: 0.9160 - binary_accuracy: 0.8358 - val_loss: 0.6754 - val_auc: 0.7641 - val_binary_accuracy: 0.5367\n\nEpoch 00002: val_auc did not improve from 0.76535\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2447 - auc: 0.9634 - binary_accuracy: 0.8982 - val_loss: 0.6830 - val_auc: 0.7490 - val_binary_accuracy: 0.5540\n\nEpoch 00003: val_auc did not improve from 0.76535\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2265 - auc: 0.9687 - binary_accuracy: 0.9074 - val_loss: 0.7407 - val_auc: 0.8356 - val_binary_accuracy: 0.5065\n\nEpoch 00004: val_auc improved from 0.76535 to 0.83557, saving model to Fold_4_3_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2035 - auc: 0.9752 - binary_accuracy: 0.9175 - val_loss: 0.5022 - val_auc: 0.9279 - val_binary_accuracy: 0.7121\n\nEpoch 00005: val_auc improved from 0.83557 to 0.92789, saving model to Fold_4_3_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1801 - auc: 0.9802 - binary_accuracy: 0.9268 - val_loss: 0.3253 - val_auc: 0.9490 - val_binary_accuracy: 0.8744\n\nEpoch 00006: val_auc improved from 0.92789 to 0.94895, saving model to Fold_4_3_weights.h5\nEpoch 7/40\n82/82 [==============================] - 7s 81ms/step - loss: 0.1691 - auc: 0.9825 - binary_accuracy: 0.9315 - val_loss: 0.3036 - val_auc: 0.9663 - val_binary_accuracy: 0.8689\n\nEpoch 00007: val_auc improved from 0.94895 to 0.96625, saving model to Fold_4_3_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1543 - auc: 0.9851 - binary_accuracy: 0.9410 - val_loss: 0.2289 - val_auc: 0.9753 - val_binary_accuracy: 0.9022\n\nEpoch 00008: val_auc improved from 0.96625 to 0.97530, saving model to Fold_4_3_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1545 - auc: 0.9853 - binary_accuracy: 0.9379 - val_loss: 0.2290 - val_auc: 0.9774 - val_binary_accuracy: 0.9051\n\nEpoch 00009: val_auc improved from 0.97530 to 0.97742, saving model to Fold_4_3_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1507 - auc: 0.9860 - binary_accuracy: 0.9391 - val_loss: 0.2266 - val_auc: 0.9756 - val_binary_accuracy: 0.9060\n\nEpoch 00010: val_auc did not improve from 0.97742\nEpoch 11/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1484 - auc: 0.9865 - binary_accuracy: 0.9406 - val_loss: 0.2062 - val_auc: 0.9805 - val_binary_accuracy: 0.9168\n\nEpoch 00011: val_auc improved from 0.97742 to 0.98047, saving model to Fold_4_3_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1516 - auc: 0.9861 - binary_accuracy: 0.9388 - val_loss: 0.2050 - val_auc: 0.9811 - val_binary_accuracy: 0.9168\n\nEpoch 00012: val_auc improved from 0.98047 to 0.98108, saving model to Fold_4_3_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1499 - auc: 0.9866 - binary_accuracy: 0.9391 - val_loss: 0.2206 - val_auc: 0.9780 - val_binary_accuracy: 0.9168\n\nEpoch 00013: val_auc did not improve from 0.98108\nEpoch 14/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1432 - auc: 0.9872 - binary_accuracy: 0.9436 - val_loss: 0.2009 - val_auc: 0.9784 - val_binary_accuracy: 0.9209\n\nEpoch 00014: val_auc did not improve from 0.98108\nEpoch 15/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1328 - auc: 0.9892 - binary_accuracy: 0.9500 - val_loss: 0.1560 - val_auc: 0.9855 - val_binary_accuracy: 0.9351\n\nEpoch 00015: val_auc improved from 0.98108 to 0.98546, saving model to Fold_4_3_weights.h5\nEpoch 16/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1392 - auc: 0.9883 - binary_accuracy: 0.9442 - val_loss: 0.1814 - val_auc: 0.9817 - val_binary_accuracy: 0.9291\n\nEpoch 00016: val_auc did not improve from 0.98546\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1265 - auc: 0.9901 - binary_accuracy: 0.9512 - val_loss: 0.1739 - val_auc: 0.9823 - val_binary_accuracy: 0.9278\n\nEpoch 00017: val_auc did not improve from 0.98546\nEpoch 18/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1293 - auc: 0.9899 - binary_accuracy: 0.9479 - val_loss: 0.1855 - val_auc: 0.9822 - val_binary_accuracy: 0.9291\n\nEpoch 00018: val_auc did not improve from 0.98546\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1275 - auc: 0.9898 - binary_accuracy: 0.9504 - val_loss: 0.1933 - val_auc: 0.9819 - val_binary_accuracy: 0.9270\n\nEpoch 00019: val_auc did not improve from 0.98546\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1011 - auc: 0.9935 - binary_accuracy: 0.9619 - val_loss: 0.2148 - val_auc: 0.9839 - val_binary_accuracy: 0.9174\n\nEpoch 00020: val_auc did not improve from 0.98546\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1016 - auc: 0.9937 - binary_accuracy: 0.9604 - val_loss: 0.1636 - val_auc: 0.9850 - val_binary_accuracy: 0.9345\n\nEpoch 00021: val_auc did not improve from 0.98546\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0923 - auc: 0.9948 - binary_accuracy: 0.9623 - val_loss: 0.1889 - val_auc: 0.9816 - val_binary_accuracy: 0.9274\n\nEpoch 00022: val_auc did not improve from 0.98546\nEpoch 23/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0894 - auc: 0.9948 - binary_accuracy: 0.9658 - val_loss: 0.2232 - val_auc: 0.9776 - val_binary_accuracy: 0.9097\n\nEpoch 00023: val_auc did not improve from 0.98546\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0860 - auc: 0.9956 - binary_accuracy: 0.9669 - val_loss: 0.2017 - val_auc: 0.9835 - val_binary_accuracy: 0.9224\n\nEpoch 00024: val_auc did not improve from 0.98546\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0853 - auc: 0.9954 - binary_accuracy: 0.9674 - val_loss: 0.1996 - val_auc: 0.9841 - val_binary_accuracy: 0.9224\n\nEpoch 00025: val_auc did not improve from 0.98546\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0833 - auc: 0.9958 - binary_accuracy: 0.9692 - val_loss: 0.1622 - val_auc: 0.9841 - val_binary_accuracy: 0.9359\n\nEpoch 00026: val_auc did not improve from 0.98546\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0801 - auc: 0.9962 - binary_accuracy: 0.9686 - val_loss: 0.1949 - val_auc: 0.9834 - val_binary_accuracy: 0.9241\n\nEpoch 00027: val_auc did not improve from 0.98546\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0858 - auc: 0.9957 - binary_accuracy: 0.9678 - val_loss: 0.1567 - val_auc: 0.9856 - val_binary_accuracy: 0.9357\n\nEpoch 00028: val_auc improved from 0.98546 to 0.98556, saving model to Fold_4_3_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0843 - auc: 0.9959 - binary_accuracy: 0.9689 - val_loss: 0.1637 - val_auc: 0.9853 - val_binary_accuracy: 0.9332\n\nEpoch 00029: val_auc did not improve from 0.98556\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0876 - auc: 0.9957 - binary_accuracy: 0.9678 - val_loss: 0.1497 - val_auc: 0.9866 - val_binary_accuracy: 0.9390\n\nEpoch 00030: val_auc improved from 0.98556 to 0.98662, saving model to Fold_4_3_weights.h5\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1002 - auc: 0.9942 - binary_accuracy: 0.9616 - val_loss: 0.1667 - val_auc: 0.9850 - val_binary_accuracy: 0.9338\n\nEpoch 00031: val_auc did not improve from 0.98662\nEpoch 32/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1113 - auc: 0.9924 - binary_accuracy: 0.9574 - val_loss: 0.1583 - val_auc: 0.9858 - val_binary_accuracy: 0.9366\n\nEpoch 00032: val_auc did not improve from 0.98662\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1112 - auc: 0.9925 - binary_accuracy: 0.9578 - val_loss: 0.1540 - val_auc: 0.9859 - val_binary_accuracy: 0.9391\n\nEpoch 00033: val_auc did not improve from 0.98662\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1151 - auc: 0.9922 - binary_accuracy: 0.9554 - val_loss: 0.1517 - val_auc: 0.9865 - val_binary_accuracy: 0.9363\n\nEpoch 00034: val_auc did not improve from 0.98662\n\nEpoch 00034: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1181 - auc: 0.9918 - binary_accuracy: 0.9544 - val_loss: 0.1594 - val_auc: 0.9856 - val_binary_accuracy: 0.9366\n\nEpoch 00035: val_auc did not improve from 0.98662\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1281 - auc: 0.9901 - binary_accuracy: 0.9506 - val_loss: 0.1565 - val_auc: 0.9856 - val_binary_accuracy: 0.9366\n\nEpoch 00036: val_auc did not improve from 0.98662\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1349 - auc: 0.9892 - binary_accuracy: 0.9496 - val_loss: 0.1629 - val_auc: 0.9851 - val_binary_accuracy: 0.9368\n\nEpoch 00037: val_auc did not improve from 0.98662\nEpoch 38/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1490 - auc: 0.9867 - binary_accuracy: 0.9438 - val_loss: 0.1664 - val_auc: 0.9843 - val_binary_accuracy: 0.9341\n\nEpoch 00038: val_auc did not improve from 0.98662\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1570 - auc: 0.9863 - binary_accuracy: 0.9385 - val_loss: 0.1784 - val_auc: 0.9820 - val_binary_accuracy: 0.9305\n\nEpoch 00039: val_auc did not improve from 0.98662\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1642 - auc: 0.9850 - binary_accuracy: 0.9363 - val_loss: 0.1955 - val_auc: 0.9787 - val_binary_accuracy: 0.9234\n\nEpoch 00040: val_auc did not improve from 0.98662\n****************************** Fold 5 Trial 1 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 297ms/step - loss: 0.5713 - auc: 0.7642 - binary_accuracy: 0.6915 - val_loss: 0.5828 - val_auc: 0.8748 - val_binary_accuracy: 0.8045\n\nEpoch 00001: val_auc improved from -inf to 0.87484, saving model to Fold_5_1_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3056 - auc: 0.9431 - binary_accuracy: 0.8697 - val_loss: 0.8561 - val_auc: 0.8974 - val_binary_accuracy: 0.6045\n\nEpoch 00002: val_auc improved from 0.87484 to 0.89742, saving model to Fold_5_1_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.2241 - auc: 0.9695 - binary_accuracy: 0.9105 - val_loss: 0.5562 - val_auc: 0.9244 - val_binary_accuracy: 0.7260\n\nEpoch 00003: val_auc improved from 0.89742 to 0.92443, saving model to Fold_5_1_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1887 - auc: 0.9783 - binary_accuracy: 0.9249 - val_loss: 0.6148 - val_auc: 0.9427 - val_binary_accuracy: 0.7239\n\nEpoch 00004: val_auc improved from 0.92443 to 0.94267, saving model to Fold_5_1_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1659 - auc: 0.9831 - binary_accuracy: 0.9352 - val_loss: 0.3935 - val_auc: 0.9662 - val_binary_accuracy: 0.8305\n\nEpoch 00005: val_auc improved from 0.94267 to 0.96624, saving model to Fold_5_1_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1604 - auc: 0.9840 - binary_accuracy: 0.9338 - val_loss: 0.2732 - val_auc: 0.9720 - val_binary_accuracy: 0.8860\n\nEpoch 00006: val_auc improved from 0.96624 to 0.97198, saving model to Fold_5_1_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 71ms/step - loss: 0.1497 - auc: 0.9864 - binary_accuracy: 0.9399 - val_loss: 0.3723 - val_auc: 0.9749 - val_binary_accuracy: 0.8413\n\nEpoch 00007: val_auc improved from 0.97198 to 0.97494, saving model to Fold_5_1_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1491 - auc: 0.9865 - binary_accuracy: 0.9407 - val_loss: 0.2204 - val_auc: 0.9765 - val_binary_accuracy: 0.9160\n\nEpoch 00008: val_auc improved from 0.97494 to 0.97646, saving model to Fold_5_1_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1331 - auc: 0.9889 - binary_accuracy: 0.9477 - val_loss: 0.4014 - val_auc: 0.9710 - val_binary_accuracy: 0.8434\n\nEpoch 00009: val_auc did not improve from 0.97646\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1305 - auc: 0.9896 - binary_accuracy: 0.9492 - val_loss: 0.2111 - val_auc: 0.9805 - val_binary_accuracy: 0.9197\n\nEpoch 00010: val_auc improved from 0.97646 to 0.98046, saving model to Fold_5_1_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1301 - auc: 0.9896 - binary_accuracy: 0.9459 - val_loss: 0.1916 - val_auc: 0.9824 - val_binary_accuracy: 0.9232\n\nEpoch 00011: val_auc improved from 0.98046 to 0.98241, saving model to Fold_5_1_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1266 - auc: 0.9900 - binary_accuracy: 0.9514 - val_loss: 0.2003 - val_auc: 0.9805 - val_binary_accuracy: 0.9209\n\nEpoch 00012: val_auc did not improve from 0.98241\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1220 - auc: 0.9910 - binary_accuracy: 0.9516 - val_loss: 0.2795 - val_auc: 0.9778 - val_binary_accuracy: 0.8964\n\nEpoch 00013: val_auc did not improve from 0.98241\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1191 - auc: 0.9912 - binary_accuracy: 0.9533 - val_loss: 0.2156 - val_auc: 0.9807 - val_binary_accuracy: 0.9164\n\nEpoch 00014: val_auc did not improve from 0.98241\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1115 - auc: 0.9922 - binary_accuracy: 0.9565 - val_loss: 0.2043 - val_auc: 0.9808 - val_binary_accuracy: 0.9203\n\nEpoch 00015: val_auc did not improve from 0.98241\n\nEpoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0865 - auc: 0.9950 - binary_accuracy: 0.9679 - val_loss: 0.1984 - val_auc: 0.9815 - val_binary_accuracy: 0.9230\n\nEpoch 00016: val_auc did not improve from 0.98241\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0800 - auc: 0.9958 - binary_accuracy: 0.9704 - val_loss: 0.2711 - val_auc: 0.9790 - val_binary_accuracy: 0.8999\n\nEpoch 00017: val_auc did not improve from 0.98241\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0728 - auc: 0.9968 - binary_accuracy: 0.9719 - val_loss: 0.2290 - val_auc: 0.9807 - val_binary_accuracy: 0.9128\n\nEpoch 00018: val_auc did not improve from 0.98241\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0763 - auc: 0.9961 - binary_accuracy: 0.9723 - val_loss: 0.3929 - val_auc: 0.9727 - val_binary_accuracy: 0.8685\n\nEpoch 00019: val_auc did not improve from 0.98241\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0692 - auc: 0.9971 - binary_accuracy: 0.9738 - val_loss: 0.1760 - val_auc: 0.9834 - val_binary_accuracy: 0.9320\n\nEpoch 00020: val_auc improved from 0.98241 to 0.98337, saving model to Fold_5_1_weights.h5\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0629 - auc: 0.9974 - binary_accuracy: 0.9782 - val_loss: 0.2182 - val_auc: 0.9824 - val_binary_accuracy: 0.9201\n\nEpoch 00021: val_auc did not improve from 0.98337\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0560 - auc: 0.9980 - binary_accuracy: 0.9810 - val_loss: 0.1906 - val_auc: 0.9834 - val_binary_accuracy: 0.9234\n\nEpoch 00022: val_auc improved from 0.98337 to 0.98343, saving model to Fold_5_1_weights.h5\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0669 - auc: 0.9971 - binary_accuracy: 0.9767 - val_loss: 0.1860 - val_auc: 0.9816 - val_binary_accuracy: 0.9322\n\nEpoch 00023: val_auc did not improve from 0.98343\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0652 - auc: 0.9974 - binary_accuracy: 0.9752 - val_loss: 0.1797 - val_auc: 0.9824 - val_binary_accuracy: 0.9309\n\nEpoch 00024: val_auc did not improve from 0.98343\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0633 - auc: 0.9977 - binary_accuracy: 0.9773 - val_loss: 0.1580 - val_auc: 0.9856 - val_binary_accuracy: 0.9388\n\nEpoch 00025: val_auc improved from 0.98343 to 0.98556, saving model to Fold_5_1_weights.h5\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0665 - auc: 0.9974 - binary_accuracy: 0.9757 - val_loss: 0.1827 - val_auc: 0.9841 - val_binary_accuracy: 0.9282\n\nEpoch 00026: val_auc did not improve from 0.98556\nEpoch 27/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0764 - auc: 0.9966 - binary_accuracy: 0.9724 - val_loss: 0.1588 - val_auc: 0.9852 - val_binary_accuracy: 0.9401\n\nEpoch 00027: val_auc did not improve from 0.98556\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0827 - auc: 0.9960 - binary_accuracy: 0.9707 - val_loss: 0.1840 - val_auc: 0.9839 - val_binary_accuracy: 0.9270\n\nEpoch 00028: val_auc did not improve from 0.98556\nEpoch 29/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0898 - auc: 0.9953 - binary_accuracy: 0.9652 - val_loss: 0.1574 - val_auc: 0.9850 - val_binary_accuracy: 0.9403\n\nEpoch 00029: val_auc did not improve from 0.98556\n\nEpoch 00029: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0957 - auc: 0.9947 - binary_accuracy: 0.9639 - val_loss: 0.1659 - val_auc: 0.9850 - val_binary_accuracy: 0.9357\n\nEpoch 00030: val_auc did not improve from 0.98556\nEpoch 31/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1016 - auc: 0.9942 - binary_accuracy: 0.9613 - val_loss: 0.1578 - val_auc: 0.9849 - val_binary_accuracy: 0.9365\n\nEpoch 00031: val_auc did not improve from 0.98556\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1226 - auc: 0.9914 - binary_accuracy: 0.9516 - val_loss: 0.1590 - val_auc: 0.9847 - val_binary_accuracy: 0.9388\n\nEpoch 00032: val_auc did not improve from 0.98556\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1236 - auc: 0.9914 - binary_accuracy: 0.9514 - val_loss: 0.1618 - val_auc: 0.9841 - val_binary_accuracy: 0.9376\n\nEpoch 00033: val_auc did not improve from 0.98556\n\nEpoch 00033: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 34/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1362 - auc: 0.9891 - binary_accuracy: 0.9489 - val_loss: 0.1784 - val_auc: 0.9820 - val_binary_accuracy: 0.9307\n\nEpoch 00034: val_auc did not improve from 0.98556\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1545 - auc: 0.9863 - binary_accuracy: 0.9409 - val_loss: 0.1904 - val_auc: 0.9799 - val_binary_accuracy: 0.9251\n\nEpoch 00035: val_auc did not improve from 0.98556\nEpoch 36/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1782 - auc: 0.9818 - binary_accuracy: 0.9307 - val_loss: 0.2124 - val_auc: 0.9780 - val_binary_accuracy: 0.9209\n\nEpoch 00036: val_auc did not improve from 0.98556\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1987 - auc: 0.9781 - binary_accuracy: 0.9220 - val_loss: 0.2217 - val_auc: 0.9779 - val_binary_accuracy: 0.9184\n\nEpoch 00037: val_auc did not improve from 0.98556\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\nEpoch 38/40\n82/82 [==============================] - 6s 74ms/step - loss: 0.2047 - auc: 0.9767 - binary_accuracy: 0.9200 - val_loss: 0.2230 - val_auc: 0.9732 - val_binary_accuracy: 0.9126\n\nEpoch 00038: val_auc did not improve from 0.98556\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2189 - auc: 0.9734 - binary_accuracy: 0.9138 - val_loss: 0.2416 - val_auc: 0.9677 - val_binary_accuracy: 0.9016\n\nEpoch 00039: val_auc did not improve from 0.98556\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2562 - auc: 0.9634 - binary_accuracy: 0.8981 - val_loss: 0.2634 - val_auc: 0.9618 - val_binary_accuracy: 0.8927\n\nEpoch 00040: val_auc did not improve from 0.98556\n****************************** Fold 5 Trial 2 ******************************\nEpoch 1/40\n82/82 [==============================] - 53s 312ms/step - loss: 0.6492 - auc: 0.7160 - binary_accuracy: 0.6592 - val_loss: 1.1086 - val_auc: 0.6035 - val_binary_accuracy: 0.5093\n\nEpoch 00001: val_auc improved from -inf to 0.60345, saving model to Fold_5_2_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.3540 - auc: 0.9209 - binary_accuracy: 0.8460 - val_loss: 2.2483 - val_auc: 0.5130 - val_binary_accuracy: 0.5005\n\nEpoch 00002: val_auc did not improve from 0.60345\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2693 - auc: 0.9559 - binary_accuracy: 0.8873 - val_loss: 2.9309 - val_auc: 0.5094 - val_binary_accuracy: 0.5001\n\nEpoch 00003: val_auc did not improve from 0.60345\nEpoch 4/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2411 - auc: 0.9642 - binary_accuracy: 0.9003 - val_loss: 2.9341 - val_auc: 0.5258 - val_binary_accuracy: 0.5005\n\nEpoch 00004: val_auc did not improve from 0.60345\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2195 - auc: 0.9707 - binary_accuracy: 0.9078 - val_loss: 2.6657 - val_auc: 0.5768 - val_binary_accuracy: 0.5074\n\nEpoch 00005: val_auc did not improve from 0.60345\n\nEpoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 6/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1827 - auc: 0.9795 - binary_accuracy: 0.9270 - val_loss: 2.6244 - val_auc: 0.6124 - val_binary_accuracy: 0.5130\n\nEpoch 00006: val_auc improved from 0.60345 to 0.61242, saving model to Fold_5_2_weights.h5\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1676 - auc: 0.9829 - binary_accuracy: 0.9318 - val_loss: 1.9852 - val_auc: 0.7917 - val_binary_accuracy: 0.5577\n\nEpoch 00007: val_auc improved from 0.61242 to 0.79165, saving model to Fold_5_2_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1703 - auc: 0.9823 - binary_accuracy: 0.9310 - val_loss: 1.3638 - val_auc: 0.8786 - val_binary_accuracy: 0.6360\n\nEpoch 00008: val_auc improved from 0.79165 to 0.87863, saving model to Fold_5_2_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1609 - auc: 0.9844 - binary_accuracy: 0.9359 - val_loss: 1.5277 - val_auc: 0.8502 - val_binary_accuracy: 0.6102\n\nEpoch 00009: val_auc did not improve from 0.87863\nEpoch 10/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1692 - auc: 0.9832 - binary_accuracy: 0.9335 - val_loss: 1.1936 - val_auc: 0.9048 - val_binary_accuracy: 0.6530\n\nEpoch 00010: val_auc improved from 0.87863 to 0.90475, saving model to Fold_5_2_weights.h5\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1495 - auc: 0.9865 - binary_accuracy: 0.9403 - val_loss: 0.6362 - val_auc: 0.9434 - val_binary_accuracy: 0.7660\n\nEpoch 00011: val_auc improved from 0.90475 to 0.94340, saving model to Fold_5_2_weights.h5\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1502 - auc: 0.9861 - binary_accuracy: 0.9404 - val_loss: 0.6034 - val_auc: 0.9456 - val_binary_accuracy: 0.8090\n\nEpoch 00012: val_auc improved from 0.94340 to 0.94563, saving model to Fold_5_2_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1505 - auc: 0.9861 - binary_accuracy: 0.9388 - val_loss: 0.4822 - val_auc: 0.9583 - val_binary_accuracy: 0.8307\n\nEpoch 00013: val_auc improved from 0.94563 to 0.95835, saving model to Fold_5_2_weights.h5\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1556 - auc: 0.9850 - binary_accuracy: 0.9390 - val_loss: 0.3336 - val_auc: 0.9651 - val_binary_accuracy: 0.8768\n\nEpoch 00014: val_auc improved from 0.95835 to 0.96513, saving model to Fold_5_2_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1329 - auc: 0.9890 - binary_accuracy: 0.9495 - val_loss: 0.4577 - val_auc: 0.9527 - val_binary_accuracy: 0.8298\n\nEpoch 00015: val_auc did not improve from 0.96513\nEpoch 16/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1372 - auc: 0.9884 - binary_accuracy: 0.9465 - val_loss: 0.4691 - val_auc: 0.9570 - val_binary_accuracy: 0.8390\n\nEpoch 00016: val_auc did not improve from 0.96513\nEpoch 17/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1348 - auc: 0.9888 - binary_accuracy: 0.9455 - val_loss: 0.2937 - val_auc: 0.9671 - val_binary_accuracy: 0.8954\n\nEpoch 00017: val_auc improved from 0.96513 to 0.96715, saving model to Fold_5_2_weights.h5\nEpoch 18/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1452 - auc: 0.9871 - binary_accuracy: 0.9414 - val_loss: 0.2273 - val_auc: 0.9745 - val_binary_accuracy: 0.9187\n\nEpoch 00018: val_auc improved from 0.96715 to 0.97453, saving model to Fold_5_2_weights.h5\nEpoch 19/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1338 - auc: 0.9892 - binary_accuracy: 0.9456 - val_loss: 0.3372 - val_auc: 0.9685 - val_binary_accuracy: 0.8839\n\nEpoch 00019: val_auc did not improve from 0.97453\nEpoch 20/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1421 - auc: 0.9879 - binary_accuracy: 0.9433 - val_loss: 0.2298 - val_auc: 0.9755 - val_binary_accuracy: 0.9124\n\nEpoch 00020: val_auc improved from 0.97453 to 0.97549, saving model to Fold_5_2_weights.h5\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1350 - auc: 0.9889 - binary_accuracy: 0.9454 - val_loss: 0.2357 - val_auc: 0.9775 - val_binary_accuracy: 0.9105\n\nEpoch 00021: val_auc improved from 0.97549 to 0.97747, saving model to Fold_5_2_weights.h5\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1450 - auc: 0.9872 - binary_accuracy: 0.9400 - val_loss: 0.2617 - val_auc: 0.9744 - val_binary_accuracy: 0.9091\n\nEpoch 00022: val_auc did not improve from 0.97747\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1370 - auc: 0.9885 - binary_accuracy: 0.9463 - val_loss: 0.2526 - val_auc: 0.9753 - val_binary_accuracy: 0.9080\n\nEpoch 00023: val_auc did not improve from 0.97747\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1246 - auc: 0.9903 - binary_accuracy: 0.9522 - val_loss: 0.2058 - val_auc: 0.9800 - val_binary_accuracy: 0.9210\n\nEpoch 00024: val_auc improved from 0.97747 to 0.98002, saving model to Fold_5_2_weights.h5\nEpoch 25/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1266 - auc: 0.9902 - binary_accuracy: 0.9486 - val_loss: 0.1990 - val_auc: 0.9787 - val_binary_accuracy: 0.9228\n\nEpoch 00025: val_auc did not improve from 0.98002\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1303 - auc: 0.9895 - binary_accuracy: 0.9498 - val_loss: 0.1920 - val_auc: 0.9787 - val_binary_accuracy: 0.9291\n\nEpoch 00026: val_auc did not improve from 0.98002\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1258 - auc: 0.9903 - binary_accuracy: 0.9517 - val_loss: 0.1826 - val_auc: 0.9817 - val_binary_accuracy: 0.9320\n\nEpoch 00027: val_auc improved from 0.98002 to 0.98167, saving model to Fold_5_2_weights.h5\nEpoch 28/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1184 - auc: 0.9914 - binary_accuracy: 0.9544 - val_loss: 0.2008 - val_auc: 0.9787 - val_binary_accuracy: 0.9178\n\nEpoch 00028: val_auc did not improve from 0.98167\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1565 - auc: 0.9851 - binary_accuracy: 0.9345 - val_loss: 0.1785 - val_auc: 0.9827 - val_binary_accuracy: 0.9320\n\nEpoch 00029: val_auc improved from 0.98167 to 0.98275, saving model to Fold_5_2_weights.h5\nEpoch 30/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1218 - auc: 0.9906 - binary_accuracy: 0.9531 - val_loss: 0.1878 - val_auc: 0.9803 - val_binary_accuracy: 0.9299\n\nEpoch 00030: val_auc did not improve from 0.98275\nEpoch 31/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1210 - auc: 0.9911 - binary_accuracy: 0.9513 - val_loss: 0.1846 - val_auc: 0.9821 - val_binary_accuracy: 0.9301\n\nEpoch 00031: val_auc did not improve from 0.98275\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1245 - auc: 0.9906 - binary_accuracy: 0.9509 - val_loss: 0.2866 - val_auc: 0.9759 - val_binary_accuracy: 0.9014\n\nEpoch 00032: val_auc did not improve from 0.98275\nEpoch 33/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1490 - auc: 0.9867 - binary_accuracy: 0.9399 - val_loss: 0.1783 - val_auc: 0.9806 - val_binary_accuracy: 0.9286\n\nEpoch 00033: val_auc did not improve from 0.98275\n\nEpoch 00033: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1119 - auc: 0.9926 - binary_accuracy: 0.9592 - val_loss: 0.1671 - val_auc: 0.9836 - val_binary_accuracy: 0.9365\n\nEpoch 00034: val_auc improved from 0.98275 to 0.98362, saving model to Fold_5_2_weights.h5\nEpoch 35/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0994 - auc: 0.9941 - binary_accuracy: 0.9631 - val_loss: 0.1633 - val_auc: 0.9847 - val_binary_accuracy: 0.9374\n\nEpoch 00035: val_auc improved from 0.98362 to 0.98468, saving model to Fold_5_2_weights.h5\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0984 - auc: 0.9941 - binary_accuracy: 0.9633 - val_loss: 0.1846 - val_auc: 0.9831 - val_binary_accuracy: 0.9318\n\nEpoch 00036: val_auc did not improve from 0.98468\nEpoch 37/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1112 - auc: 0.9925 - binary_accuracy: 0.9572 - val_loss: 0.1658 - val_auc: 0.9837 - val_binary_accuracy: 0.9390\n\nEpoch 00037: val_auc did not improve from 0.98468\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1129 - auc: 0.9924 - binary_accuracy: 0.9569 - val_loss: 0.1668 - val_auc: 0.9841 - val_binary_accuracy: 0.9384\n\nEpoch 00038: val_auc did not improve from 0.98468\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1093 - auc: 0.9927 - binary_accuracy: 0.9571 - val_loss: 0.1668 - val_auc: 0.9830 - val_binary_accuracy: 0.9363\n\nEpoch 00039: val_auc did not improve from 0.98468\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1049 - auc: 0.9936 - binary_accuracy: 0.9588 - val_loss: 0.1609 - val_auc: 0.9840 - val_binary_accuracy: 0.9390\n\nEpoch 00040: val_auc did not improve from 0.98468\n****************************** Fold 5 Trial 3 ******************************\nEpoch 1/40\n82/82 [==============================] - 54s 297ms/step - loss: 0.6335 - auc: 0.7286 - binary_accuracy: 0.6745 - val_loss: 0.9522 - val_auc: 0.8399 - val_binary_accuracy: 0.5328\n\nEpoch 00001: val_auc improved from -inf to 0.83986, saving model to Fold_5_3_weights.h5\nEpoch 2/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.3024 - auc: 0.9436 - binary_accuracy: 0.8705 - val_loss: 0.8121 - val_auc: 0.8591 - val_binary_accuracy: 0.5584\n\nEpoch 00002: val_auc improved from 0.83986 to 0.85911, saving model to Fold_5_3_weights.h5\nEpoch 3/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.2103 - auc: 0.9729 - binary_accuracy: 0.9130 - val_loss: 0.7387 - val_auc: 0.9300 - val_binary_accuracy: 0.6580\n\nEpoch 00003: val_auc improved from 0.85911 to 0.93003, saving model to Fold_5_3_weights.h5\nEpoch 4/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.2041 - auc: 0.9750 - binary_accuracy: 0.9167 - val_loss: 0.6705 - val_auc: 0.9362 - val_binary_accuracy: 0.6907\n\nEpoch 00004: val_auc improved from 0.93003 to 0.93617, saving model to Fold_5_3_weights.h5\nEpoch 5/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1649 - auc: 0.9837 - binary_accuracy: 0.9346 - val_loss: 0.2988 - val_auc: 0.9572 - val_binary_accuracy: 0.8704\n\nEpoch 00005: val_auc improved from 0.93617 to 0.95717, saving model to Fold_5_3_weights.h5\nEpoch 6/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.1789 - auc: 0.9808 - binary_accuracy: 0.9293 - val_loss: 0.4053 - val_auc: 0.9567 - val_binary_accuracy: 0.8302\n\nEpoch 00006: val_auc did not improve from 0.95717\nEpoch 7/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1624 - auc: 0.9836 - binary_accuracy: 0.9374 - val_loss: 0.2082 - val_auc: 0.9785 - val_binary_accuracy: 0.9162\n\nEpoch 00007: val_auc improved from 0.95717 to 0.97853, saving model to Fold_5_3_weights.h5\nEpoch 8/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1469 - auc: 0.9869 - binary_accuracy: 0.9406 - val_loss: 0.2018 - val_auc: 0.9789 - val_binary_accuracy: 0.9172\n\nEpoch 00008: val_auc improved from 0.97853 to 0.97894, saving model to Fold_5_3_weights.h5\nEpoch 9/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1398 - auc: 0.9880 - binary_accuracy: 0.9447 - val_loss: 0.1776 - val_auc: 0.9813 - val_binary_accuracy: 0.9293\n\nEpoch 00009: val_auc improved from 0.97894 to 0.98131, saving model to Fold_5_3_weights.h5\nEpoch 10/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1415 - auc: 0.9881 - binary_accuracy: 0.9423 - val_loss: 0.2424 - val_auc: 0.9750 - val_binary_accuracy: 0.9039\n\nEpoch 00010: val_auc did not improve from 0.98131\nEpoch 11/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1334 - auc: 0.9889 - binary_accuracy: 0.9485 - val_loss: 0.1878 - val_auc: 0.9810 - val_binary_accuracy: 0.9247\n\nEpoch 00011: val_auc did not improve from 0.98131\nEpoch 12/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1254 - auc: 0.9901 - binary_accuracy: 0.9524 - val_loss: 0.1750 - val_auc: 0.9831 - val_binary_accuracy: 0.9332\n\nEpoch 00012: val_auc improved from 0.98131 to 0.98311, saving model to Fold_5_3_weights.h5\nEpoch 13/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1198 - auc: 0.9910 - binary_accuracy: 0.9516 - val_loss: 0.1925 - val_auc: 0.9804 - val_binary_accuracy: 0.9297\n\nEpoch 00013: val_auc did not improve from 0.98311\nEpoch 14/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1325 - auc: 0.9895 - binary_accuracy: 0.9458 - val_loss: 0.1884 - val_auc: 0.9840 - val_binary_accuracy: 0.9220\n\nEpoch 00014: val_auc improved from 0.98311 to 0.98402, saving model to Fold_5_3_weights.h5\nEpoch 15/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1344 - auc: 0.9889 - binary_accuracy: 0.9456 - val_loss: 0.1636 - val_auc: 0.9843 - val_binary_accuracy: 0.9384\n\nEpoch 00015: val_auc improved from 0.98402 to 0.98427, saving model to Fold_5_3_weights.h5\nEpoch 16/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1175 - auc: 0.9914 - binary_accuracy: 0.9544 - val_loss: 0.1849 - val_auc: 0.9820 - val_binary_accuracy: 0.9297\n\nEpoch 00016: val_auc did not improve from 0.98427\nEpoch 17/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1346 - auc: 0.9890 - binary_accuracy: 0.9452 - val_loss: 0.2356 - val_auc: 0.9814 - val_binary_accuracy: 0.9114\n\nEpoch 00017: val_auc did not improve from 0.98427\nEpoch 18/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1262 - auc: 0.9905 - binary_accuracy: 0.9493 - val_loss: 0.2031 - val_auc: 0.9830 - val_binary_accuracy: 0.9232\n\nEpoch 00018: val_auc did not improve from 0.98427\nEpoch 19/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.1038 - auc: 0.9933 - binary_accuracy: 0.9600 - val_loss: 0.1853 - val_auc: 0.9812 - val_binary_accuracy: 0.9295\n\nEpoch 00019: val_auc did not improve from 0.98427\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 20/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0949 - auc: 0.9947 - binary_accuracy: 0.9614 - val_loss: 0.1850 - val_auc: 0.9846 - val_binary_accuracy: 0.9311\n\nEpoch 00020: val_auc improved from 0.98427 to 0.98463, saving model to Fold_5_3_weights.h5\nEpoch 21/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0753 - auc: 0.9965 - binary_accuracy: 0.9705 - val_loss: 0.1923 - val_auc: 0.9809 - val_binary_accuracy: 0.9289\n\nEpoch 00021: val_auc did not improve from 0.98463\nEpoch 22/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0881 - auc: 0.9951 - binary_accuracy: 0.9657 - val_loss: 0.2070 - val_auc: 0.9817 - val_binary_accuracy: 0.9199\n\nEpoch 00022: val_auc did not improve from 0.98463\nEpoch 23/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0746 - auc: 0.9966 - binary_accuracy: 0.9708 - val_loss: 0.1896 - val_auc: 0.9827 - val_binary_accuracy: 0.9289\n\nEpoch 00023: val_auc did not improve from 0.98463\nEpoch 24/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0755 - auc: 0.9963 - binary_accuracy: 0.9704 - val_loss: 0.1743 - val_auc: 0.9834 - val_binary_accuracy: 0.9347\n\nEpoch 00024: val_auc did not improve from 0.98463\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 25/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0685 - auc: 0.9973 - binary_accuracy: 0.9746 - val_loss: 0.1852 - val_auc: 0.9840 - val_binary_accuracy: 0.9289\n\nEpoch 00025: val_auc did not improve from 0.98463\nEpoch 26/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0602 - auc: 0.9979 - binary_accuracy: 0.9789 - val_loss: 0.1821 - val_auc: 0.9817 - val_binary_accuracy: 0.9332\n\nEpoch 00026: val_auc did not improve from 0.98463\nEpoch 27/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0649 - auc: 0.9975 - binary_accuracy: 0.9757 - val_loss: 0.1688 - val_auc: 0.9844 - val_binary_accuracy: 0.9347\n\nEpoch 00027: val_auc did not improve from 0.98463\nEpoch 28/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0654 - auc: 0.9975 - binary_accuracy: 0.9763 - val_loss: 0.1658 - val_auc: 0.9861 - val_binary_accuracy: 0.9399\n\nEpoch 00028: val_auc improved from 0.98463 to 0.98613, saving model to Fold_5_3_weights.h5\nEpoch 29/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0733 - auc: 0.9969 - binary_accuracy: 0.9723 - val_loss: 0.1624 - val_auc: 0.9851 - val_binary_accuracy: 0.9409\n\nEpoch 00029: val_auc did not improve from 0.98613\nEpoch 30/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0686 - auc: 0.9973 - binary_accuracy: 0.9746 - val_loss: 0.1605 - val_auc: 0.9849 - val_binary_accuracy: 0.9413\n\nEpoch 00030: val_auc did not improve from 0.98613\nEpoch 31/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0791 - auc: 0.9964 - binary_accuracy: 0.9676 - val_loss: 0.1698 - val_auc: 0.9846 - val_binary_accuracy: 0.9378\n\nEpoch 00031: val_auc did not improve from 0.98613\nEpoch 32/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0752 - auc: 0.9966 - binary_accuracy: 0.9738 - val_loss: 0.1685 - val_auc: 0.9850 - val_binary_accuracy: 0.9413\n\nEpoch 00032: val_auc did not improve from 0.98613\n\nEpoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 33/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0748 - auc: 0.9966 - binary_accuracy: 0.9736 - val_loss: 0.1505 - val_auc: 0.9864 - val_binary_accuracy: 0.9436\n\nEpoch 00033: val_auc improved from 0.98613 to 0.98636, saving model to Fold_5_3_weights.h5\nEpoch 34/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0799 - auc: 0.9963 - binary_accuracy: 0.9698 - val_loss: 0.1586 - val_auc: 0.9859 - val_binary_accuracy: 0.9415\n\nEpoch 00034: val_auc did not improve from 0.98636\nEpoch 35/40\n82/82 [==============================] - 6s 70ms/step - loss: 0.0847 - auc: 0.9961 - binary_accuracy: 0.9655 - val_loss: 0.1496 - val_auc: 0.9863 - val_binary_accuracy: 0.9442\n\nEpoch 00035: val_auc did not improve from 0.98636\nEpoch 36/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.0891 - auc: 0.9954 - binary_accuracy: 0.9663 - val_loss: 0.1495 - val_auc: 0.9866 - val_binary_accuracy: 0.9432\n\nEpoch 00036: val_auc improved from 0.98636 to 0.98656, saving model to Fold_5_3_weights.h5\nEpoch 37/40\n82/82 [==============================] - 6s 68ms/step - loss: 0.0962 - auc: 0.9946 - binary_accuracy: 0.9612 - val_loss: 0.1691 - val_auc: 0.9851 - val_binary_accuracy: 0.9355\n\nEpoch 00037: val_auc did not improve from 0.98656\nEpoch 38/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1040 - auc: 0.9939 - binary_accuracy: 0.9572 - val_loss: 0.1600 - val_auc: 0.9855 - val_binary_accuracy: 0.9417\n\nEpoch 00038: val_auc did not improve from 0.98656\nEpoch 39/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1106 - auc: 0.9928 - binary_accuracy: 0.9554 - val_loss: 0.1448 - val_auc: 0.9871 - val_binary_accuracy: 0.9455\n\nEpoch 00039: val_auc improved from 0.98656 to 0.98708, saving model to Fold_5_3_weights.h5\nEpoch 40/40\n82/82 [==============================] - 6s 69ms/step - loss: 0.1165 - auc: 0.9921 - binary_accuracy: 0.9554 - val_loss: 0.1573 - val_auc: 0.9853 - val_binary_accuracy: 0.9399\n\nEpoch 00040: val_auc did not improve from 0.98708\n","output_type":"stream"}]},{"cell_type":"code","source":"best = [1, 2, 2, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:20:58.643180Z","iopub.execute_input":"2022-04-29T23:20:58.644511Z","iopub.status.idle":"2022-04-29T23:20:58.650338Z","shell.execute_reply.started":"2022-04-29T23:20:58.644351Z","shell.execute_reply":"2022-04-29T23:20:58.649426Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train_1, x_train_2, y_train = train_scaled[train_idx], train_seq_df[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid_1, x_valid_2, y_valid = train_scaled[valid_idx], train_seq_df[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=4, verbose=True, mode='max')\n#     with strategy.scope():\n#         model = lstm_model()\n#     model.fit((x_train_1, x_train_2), y_train, \n#               validation_data=((x_valid_1, x_valid_2), y_valid),\n#               epochs=20, batch_size=256, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T14:37:54.268397Z","iopub.execute_input":"2022-04-29T14:37:54.26889Z","iopub.status.idle":"2022-04-29T14:37:54.280123Z","shell.execute_reply.started":"2022-04-29T14:37:54.268843Z","shell.execute_reply":"2022-04-29T14:37:54.279411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_preds_array = np.zeros(len(train_scaled))\ntest_preds_array = np.zeros(len(test_scaled))\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n    print('*'*30, f'Fold {fold+1}', '*'*30)\n    x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n    x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        model = lstm_model()\n        print('Weights:', f'./Fold_{fold+1}_{best[fold]}_weights.h5')\n        model.load_weights(f'./Fold_{fold+1}_{best[fold]}_weights.h5')\n    valid_preds = model.predict(x_valid, batch_size=256, verbose=True)\n    model.evaluate(x=x_valid, y=y_valid, batch_size=256, verbose=True)\n    valid_preds_array[valid_idx] = np.squeeze(valid_preds)\n\n    test_preds = model.predict(test_scaled, batch_size=256, verbose=True)\n    test_preds_array += np.squeeze(test_preds) / n_splits","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:21:09.401440Z","iopub.execute_input":"2022-04-29T23:21:09.402594Z","iopub.status.idle":"2022-04-29T23:23:16.150268Z","shell.execute_reply.started":"2022-04-29T23:21:09.402509Z","shell.execute_reply":"2022-04-29T23:23:16.148640Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"****************************** Fold 1 ******************************\nWeights: ./Fold_1_1_weights.h5\n21/21 [==============================] - 8s 257ms/step\n21/21 [==============================] - 8s 129ms/step - loss: 0.1599 - auc: 0.9851 - binary_accuracy: 0.9370\n48/48 [==============================] - 4s 72ms/step\n****************************** Fold 2 ******************************\nWeights: ./Fold_2_2_weights.h5\n21/21 [==============================] - 7s 255ms/step\n21/21 [==============================] - 8s 133ms/step - loss: 0.1552 - auc: 0.9861 - binary_accuracy: 0.9431\n48/48 [==============================] - 4s 67ms/step\n****************************** Fold 3 ******************************\nWeights: ./Fold_3_2_weights.h5\n21/21 [==============================] - 7s 255ms/step\n21/21 [==============================] - 9s 129ms/step - loss: 0.1480 - auc: 0.9874 - binary_accuracy: 0.9456\n48/48 [==============================] - 4s 69ms/step\n****************************** Fold 4 ******************************\nWeights: ./Fold_4_2_weights.h5\n21/21 [==============================] - 7s 251ms/step\n21/21 [==============================] - 8s 134ms/step - loss: 0.1469 - auc: 0.9873 - binary_accuracy: 0.9419\n48/48 [==============================] - 4s 70ms/step\n****************************** Fold 5 ******************************\nWeights: ./Fold_5_3_weights.h5\n21/21 [==============================] - 7s 250ms/step\n21/21 [==============================] - 8s 131ms/step - loss: 0.1425 - auc: 0.9878 - binary_accuracy: 0.9448\n48/48 [==============================] - 3s 67ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"np.mean([0.9851, 0.9861, 0.9874, 0.9873, 0.9878])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:23:44.221682Z","iopub.execute_input":"2022-04-29T23:23:44.222616Z","iopub.status.idle":"2022-04-29T23:23:44.233725Z","shell.execute_reply.started":"2022-04-29T23:23:44.222568Z","shell.execute_reply":"2022-04-29T23:23:44.232975Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.98674"},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame({'preds': valid_preds_array}).to_csv('valid_preds_array_9867.csv', index=False)\npd.DataFrame({'preds': test_preds_array}).to_csv('test_preds_array_9867.csv', index=False)\n# # !rm ./Fold*","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:24:04.641767Z","iopub.execute_input":"2022-04-29T23:24:04.642047Z","iopub.status.idle":"2022-04-29T23:24:04.793090Z","shell.execute_reply.started":"2022-04-29T23:24:04.642017Z","shell.execute_reply":"2022-04-29T23:24:04.792246Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# 256 ----> 0.9591\n# 512 ----> 0.9612\n# 512, 4, 64, 4, 0.0 ----> 0.9637\n# 0.9689","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.327092Z","iopub.execute_input":"2022-04-29T14:37:54.327424Z","iopub.status.idle":"2022-04-29T14:37:54.336916Z","shell.execute_reply.started":"2022-04-29T14:37:54.327381Z","shell.execute_reply":"2022-04-29T14:37:54.335975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('./Fold_1_weights.h5')\n# preds = model.predict(test_scaled, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.338247Z","iopub.execute_input":"2022-04-29T14:37:54.338483Z","iopub.status.idle":"2022-04-29T14:37:54.348961Z","shell.execute_reply.started":"2022-04-29T14:37:54.338456Z","shell.execute_reply":"2022-04-29T14:37:54.347741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample.iloc[:, -1] = preds.reshape(-1,)\n# sample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.350757Z","iopub.execute_input":"2022-04-29T14:37:54.35143Z","iopub.status.idle":"2022-04-29T14:37:54.360588Z","shell.execute_reply.started":"2022-04-29T14:37:54.351381Z","shell.execute_reply":"2022-04-29T14:37:54.359979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}